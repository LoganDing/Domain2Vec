{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw/20131228'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_domain_flow_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw/20131228/user_domain_flow\"\n",
    "os.path.split(user_domain_flow_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-repeat-model-size2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import shutil \n",
    "LOG_FILE = \"train.log\"\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename = LOG_FILE)\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "#             if fname == (\"part-00001\"):\n",
    "#                 for line in open(os.path.join(self.dirname, fname)):\n",
    "#                     yield line.split()\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "                \n",
    "def train_word2vec(root_path):\n",
    "    input_data_path = os.path.join(root_path, \"domain_seq_distinct\")\n",
    "    output_model_path = os.path.join(root_path, \"ModelResult\")\n",
    "    if not os.path.exists(input_data_path):\n",
    "        print(input_path, \"not exists!!!\")\n",
    "    sentences = MySentences(input_data_path) # a memory-friendly iterator\n",
    "    \n",
    "    if not os.path.exists(output_model_path):\n",
    "        os.mkdir(output_model_path)\n",
    "    \n",
    "    for s in [2,4,8,16,32,64,128,256,512,1024,2048]:\n",
    "        print(\"train-repeat-model-size%s\"%str(s))\n",
    "        one_model_path = output_model_path + \"/model-size%s\"%str(s)\n",
    "        if not os.path.exists(one_model_path):\n",
    "            os.mkdir(one_model_path)\n",
    "        model = gensim.models.Word2Vec(sentences,size=s, window=9, min_count=8, workers=500, sg = 1)\n",
    "        model.save_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "\n",
    "#     for w in [1,2,3,5,7,9,11,13,15,20,25,30,40,50]:\n",
    "#         model = gensim.models.Word2Vec(sentences,size=100, window=w, min_count=8, workers=500, sg = 1)\n",
    "#         one_model_path = output_model_path + \"/model-window%s\"%str(w)\n",
    "#         if not os.path.exists(one_model_path):\n",
    "#             os.mkdir(one_model_path)        \n",
    "#         model.save_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "        \n",
    "user_domain_flow_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw/20131228/user_domain_flow\"\n",
    "train_word2vec(os.path.split(user_domain_flow_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import shutil \n",
    "LOG_FILE = \"20131229-20140103.log\"\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename = LOG_FILE)\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "#             if fname == (\"part-00001\"):\n",
    "#                 for line in open(os.path.join(self.dirname, fname)):\n",
    "#                     yield line.split()\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "                \n",
    "def train_word2vec(root_path):\n",
    "    input_data_path = os.path.join(root_path, \"domain_seq_distinct\")\n",
    "    output_model_path = os.path.join(root_path, \"ModelResult\")\n",
    "    if not os.path.exists(input_data_path):\n",
    "        print(input_path, \"not exists!!!\")\n",
    "    sentences = MySentences(input_data_path) # a memory-friendly iterator\n",
    "    \n",
    "    if not os.path.exists(output_model_path):\n",
    "        os.mkdir(output_model_path)\n",
    "    \n",
    "    for s in [100]:\n",
    "        print(\"train-repeat-model-size%s\"%str(s))\n",
    "        one_model_path = output_model_path + \"/model-size%s\"%str(s)\n",
    "        if not os.path.exists(one_model_path):\n",
    "            os.mkdir(one_model_path)\n",
    "        model = gensim.models.Word2Vec(sentences,size=s, window=9, min_count=8, workers=500, sg = 1)\n",
    "        model.save_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "\n",
    "#     for w in [1,2,3,5,7,9,11,13,15,20,25,30,40,50]:\n",
    "#         model = gensim.models.Word2Vec(sentences,size=100, window=w, min_count=8, workers=500, sg = 1)\n",
    "#         one_model_path = output_model_path + \"/model-window%s\"%str(w)\n",
    "#         if not os.path.exists(one_model_path):\n",
    "#             os.mkdir(one_model_path)        \n",
    "#         model.save_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "        \n",
    "# user_domain_flow_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw/20131228/user_domain_flow\"\n",
    "# train_word2vec(os.path.split(user_domain_flow_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20131229\n",
      "train-repeat-model-size100\n"
     ]
    }
   ],
   "source": [
    "for i in [20131229,20131230,20131231,20140101,20140102,20140103]:\n",
    "    user_domain_flow_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw/%s/user_domain_flow\"%str(i)\n",
    "    print(i)\n",
    "    train_word2vec(os.path.split(user_domain_flow_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
