{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model size:100\n",
      "[ 0.65853758  0.82090548  0.84987636  0.78773452  0.88589086  0.88524239\n",
      "  0.85856777  0.87514048  0.92560194  0.86144689  0.84573382  0.92427945\n",
      "  0.8630863   0.92459884  0.90818815  0.88722578  0.86269841  0.81680636\n",
      "  0.83398833  0.87291155]\n",
      "Accuracy: 0.86 (+/- 0.12) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for date in [20131228]:\n",
    "    # for s in [1,2,3,5,7,9,13,15,20,30,50]:\n",
    "    for i in [100]:\n",
    "        print(\"evaluate model size:%s\"%str(i))\n",
    "        one_model_path = os.path.join(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw\",str(date)+\"/ModelResult/model-size%s\"%str(i))\n",
    "        model = gensim.models.Word2Vec.load_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "        count = 400\n",
    "        shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "        map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "        media_embeddings = [model[word] for word in model.index2word  if \"mail\" in word]\n",
    "        game_embeddings = [model[word] for word in model.index2word  if \"game\" in word]\n",
    "\n",
    "        y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)]+[3 for i in range(count)])\n",
    "        X = shop_embeddings[:count]+map_embeddings[:count]+media_embeddings[:count]+game_embeddings[:count]\n",
    "\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "        SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "        scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "        print(scores)                                              \n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\"\\n\")\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-repeat-model-windows128\n",
      "train-repeat-model-windows256\n",
      "train-repeat-model-windows512\n",
      "train-repeat-model-windows1024\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import shutil \n",
    "LOG_FILE = \"train_windows.log\"\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename = LOG_FILE)\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "                \n",
    "def train_word2vec(root_path):\n",
    "    input_data_path = os.path.join(root_path, \"domain_seq_distinct\")\n",
    "    output_model_path = os.path.join(root_path, \"ModelResult\")\n",
    "    if not os.path.exists(input_data_path):\n",
    "        print(input_path, \"not exists!!!\")\n",
    "    sentences = MySentences(input_data_path) # a memory-friendly iterator\n",
    "    \n",
    "    if not os.path.exists(output_model_path):\n",
    "        os.mkdir(output_model_path)\n",
    "    \n",
    "#     for s in [2,4,8,16,32,64,128,256,512,1024,2048]:\n",
    "#         print(\"train-repeat-model-size%s\"%str(s))\n",
    "#         one_model_path = output_model_path + \"/model-size%s\"%str(s)\n",
    "#         if not os.path.exists(one_model_path):\n",
    "#             os.mkdir(one_model_path)\n",
    "#         model = gensim.models.Word2Vec(sentences,size=s, window=9, min_count=8, workers=500, sg = 1)\n",
    "#         model.save_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "\n",
    "    for w in [128,256,512,1024]:\n",
    "        print(\"train-repeat-model-windows%s\"%str(w))\n",
    "        one_model_path = output_model_path + \"/model-window%s\"%str(w)\n",
    "        if not os.path.exists(one_model_path):\n",
    "            os.mkdir(one_model_path)   \n",
    "        model = gensim.models.Word2Vec(sentences,size=100, window=w, min_count=8, workers=500, sg = 1)\n",
    "        model.save_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "        \n",
    "user_domain_flow_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw/20131228/user_domain_flow\"\n",
    "train_word2vec(os.path.split(user_domain_flow_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model size:2\n",
      "[ 0.40746269  0.36842105  0.34736842]\n",
      "Accuracy: 0.37 (+/- 0.05) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for date in [20131228]:\n",
    "    # for s in [1,2,3,5,7,9,13,15,20,30,50]:\n",
    "#     for i in [2,4,8,16,32,64,128,256,512]:\n",
    "    for i in [2]:\n",
    "        print(\"evaluate model size:%s\"%str(i))\n",
    "        one_model_path = os.path.join(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/domain2vec_jw\",str(date)+\"/ModelResult/model-size%s\"%str(i))\n",
    "        model = gensim.models.Word2Vec.load_word2vec_format(one_model_path+\"/model-all-word2vec\", fvocab=one_model_path+\"/model-all-vocabulary\", binary=False)\n",
    "        count = 400\n",
    "        baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "        taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "        qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]      \n",
    "        sina_embeddings = [model[word] for word in model.index2word  if \"sina\" in word]\n",
    "        wangyi_embeddings = [model[word] for word in model.index2word  if \"163\" in word]\n",
    "#         uc_embeddings = [model[word] for word in model.index2word  if \"uc\" in word]\n",
    "\n",
    "        y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)]+[3 for i in range(count)]+[4 for i in range(count)])\n",
    "        X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count] + sina_embeddings[:count]+wangyi_embeddings[:count]\n",
    "\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "        SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "        scores = cross_val_score(SVM, X, y, cv=3,scoring='f1_macro')\n",
    "        print(scores)                                              \n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\"\\n\")\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
