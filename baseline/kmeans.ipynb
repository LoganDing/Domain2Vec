{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size100\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#for i in [100,200,500,1000,2000]:\n",
    "for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "uc_embeddings = [model[word] for word in model.index2word  if \"uc\" in word]\n",
    "umeng_embeddings = [model[word] for word in model.index2word  if \"umeng\" in word]\n",
    "sina_embeddings = [model[word] for word in model.index2word  if \"sina\" in word]\n",
    "taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "qiyi_embeddings = [model[word] for word in model.index2word  if \"qiyi\" in word]\n",
    "wangyi_embeddings = [model[word] for word in model.index2word  if \"163.\" in word]\n",
    "video_embeddings = [model[word] for word in model.index2word  if \"video\" in word]\n",
    "music_embeddings = [model[word] for word in model.index2word  if \"music\" in word]\n",
    "game_embeddings = [model[word] for word in model.index2word  if \"game\" in word]\n",
    "news_embeddings = [model[word] for word in model.index2word  if \"news\" in word]\n",
    "book_embeddings = [model[word] for word in model.index2word  if \"book\" in word]\n",
    "apps_embeddings = [model[word] for word in model.index2word  if \"apps\" in word]\n",
    "img_embeddings = [model[word] for word in model.index2word  if \"img\" in word]\n",
    "shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "pay_embeddings = [model[word] for word in model.index2word  if \"pay\" in word]\n",
    "disk_embeddings = [model[word] for word in model.index2word  if \"disk\" in word]\n",
    "yun_embeddings = [model[word] for word in model.index2word  if \"yun\" in word]\n",
    "map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "meituan_embeddings = [model[word] for word in model.index2word  if \"meituan\" in word]\n",
    "weibo_embeddings = [model[word] for word in model.index2word  if \"weibo\" in word]\n",
    "chat_embeddings = [model[word] for word in model.index2word  if \"chat\" in word]\n",
    "download_embeddings = [model[word] for word in model.index2word  if \"download\" in word]\n",
    "api_embeddings = [model[word] for word in model.index2word  if \"api\" in word]\n",
    "youxi_embeddings = [model[word] for word in model.index2word  if \"youxi\" in word or \"game\" in word ]\n",
    "media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "travel_embeddings = [model[word] for word in model.index2word  if \"travel\" in word or \"trip\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aliyun_embeddings = [model[word] for word in model.index2word  if \"aliyun\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ya.tmall.com',\n",
       " 'm.tmall.com',\n",
       " 'jdy.tmall.com',\n",
       " 'a.m.tmall.com',\n",
       " 's.click.tmall.com',\n",
       " 'www.tmall.com',\n",
       " 's.m.tmall.com',\n",
       " 'smc.tmall.com',\n",
       " 'cart.m.tmall.com',\n",
       " 'login.m.tmall.com',\n",
       " 'pcookie.tmall.com',\n",
       " 'page.m.tmall.com',\n",
       " 'yyz.m.tmall.com',\n",
       " 'detail.tmall.com',\n",
       " 'twp.m.tmall.com',\n",
       " 'go.m.tmall.com',\n",
       " 'list.tmall.com',\n",
       " 'pass.tmall.com',\n",
       " 'buy.m.tmall.com',\n",
       " 'bar.tmall.com',\n",
       " 'chaoshi.m.tmall.com',\n",
       " 'aldcdn.tmall.com',\n",
       " 'webww.tmall.com',\n",
       " 'err.tmall.com',\n",
       " 'mdetail.tmall.com',\n",
       " 'dsr.rate.tmall.com',\n",
       " 'api.m.tmall.com',\n",
       " 'rate.tmall.com',\n",
       " 'shop.m.tmall.com',\n",
       " 'fav.m.tmall.com',\n",
       " 'buy.tmall.com',\n",
       " 'page.twp.tmall.com',\n",
       " 'mybrand.tmall.com',\n",
       " 'vip.tmall.com',\n",
       " 'gongxiao.tmall.com',\n",
       " 'tuikuan.tmall.com',\n",
       " 'vivo.m.tmall.com',\n",
       " 'uniqlo.m.tmall.com',\n",
       " 'neo.tmall.com',\n",
       " 'trade.tmall.com',\n",
       " 'levis.tmall.com',\n",
       " 'z.tmall008.com',\n",
       " 'd.m.tmall.com',\n",
       " 'fbuy.tmall.com',\n",
       " 'only.m.tmall.com',\n",
       " 'levis.m.tmall.com',\n",
       " 'cart.tmall.com',\n",
       " 'www.025tmall.com',\n",
       " 'detail.m.tmall.com',\n",
       " 'semir.m.tmall.com',\n",
       " 'nike.m.tmall.com',\n",
       " 'chaoshi.tmall.com',\n",
       " 'handuyishe.m.tmall.com',\n",
       " 'brand.m.tmall.com',\n",
       " 'adidas.m.tmall.com',\n",
       " 'metersbonwe.m.tmall.com',\n",
       " 'brand.tmall.com',\n",
       " 'hf.m.tmall.com',\n",
       " 'sanzhisongshu.m.tmall.com',\n",
       " 'romon.m.tmall.com',\n",
       " 'jackjones.m.tmall.com',\n",
       " 'r.m.tmall.com',\n",
       " 'refund.tmall.com',\n",
       " 'yztsyx.m.tmall.com',\n",
       " 'inman.m.tmall.com',\n",
       " 'laiyifen.m.tmall.com',\n",
       " 'etam.m.tmall.com',\n",
       " 'newbalance.m.tmall.com',\n",
       " 'qiushuiyiren.m.tmall.com',\n",
       " 'ripfs.m.tmall.com',\n",
       " 'jumbougg.m.tmall.com',\n",
       " 'ruiao.m.tmall.com',\n",
       " 'miroy.m.tmall.com',\n",
       " 'balabala.m.tmall.com',\n",
       " 'jeanswest.m.tmall.com',\n",
       " 'gap.m.tmall.com',\n",
       " 'ratewrite.tmall.com',\n",
       " 'chuyu.m.tmall.com',\n",
       " 'piaolingdashu.m.tmall.com',\n",
       " 'yunnantechan.m.tmall.com',\n",
       " 'pouillylegende.m.tmall.com',\n",
       " 'pg.tmall.com',\n",
       " 'threecolour.m.tmall.com',\n",
       " 'jinyuannz.m.tmall.com',\n",
       " 'artka.m.tmall.com',\n",
       " 'sentubila.m.tmall.com',\n",
       " 'meadjohnson.tmall.com',\n",
       " '189.m.tmall.com',\n",
       " 'fiveplus.m.tmall.com',\n",
       " 'decathlon.m.tmall.com',\n",
       " 'nanjirentl.m.tmall.com',\n",
       " 'ferrero.m.tmall.com',\n",
       " 'binf.m.tmall.com',\n",
       " 'login.tmall.com',\n",
       " 'mobile.tmall.com',\n",
       " 'veromoda.m.tmall.com',\n",
       " 'teenieweenie.m.tmall.com',\n",
       " 'hengyuanxiangbby.tmall.com',\n",
       " 'brand1.tmall.com',\n",
       " 'tonlion.m.tmall.com',\n",
       " 'tiantian.m.tmall.com',\n",
       " '361du.m.tmall.com',\n",
       " 'ochirly.m.tmall.com',\n",
       " 'xpyd.m.tmall.com',\n",
       " 'qianmeiyuan.m.tmall.com',\n",
       " 'lining.m.tmall.com',\n",
       " 'nongge.m.tmall.com',\n",
       " 'pg.m.tmall.com',\n",
       " 'ebelle.m.tmall.com',\n",
       " 'vivo.tmall.com',\n",
       " 'mudifs.m.tmall.com',\n",
       " 'shinenany.m.tmall.com',\n",
       " 'fruitday.tmall.com',\n",
       " 'cz.m.tmall.com',\n",
       " '1727.m.tmall.com',\n",
       " 'rights.tmall.com',\n",
       " 'loreal.m.tmall.com',\n",
       " 'yaojingdekoudai.m.tmall.com',\n",
       " 'jinyuejj.m.tmall.com',\n",
       " 'camelnz.m.tmall.com',\n",
       " 'me-city.m.tmall.com',\n",
       " 'sffs.m.tmall.com',\n",
       " 'ruibeika.m.tmall.com',\n",
       " 'heilanhome.m.tmall.com',\n",
       " 'daphne.m.tmall.com',\n",
       " 'thefaceshop.m.tmall.com',\n",
       " 'auman.m.tmall.com',\n",
       " 'chaoshi.detail.tmall.com',\n",
       " 'langsha.m.tmall.com',\n",
       " 'ttwlswysp.m.tmall.com',\n",
       " 'gmlsp.m.tmall.com',\n",
       " 'yishengyuan.m.tmall.com',\n",
       " 'estii.m.tmall.com',\n",
       " 'luotuo.m.tmall.com',\n",
       " 'edenpure.m.tmall.com',\n",
       " 'fotile.m.tmall.com',\n",
       " 'bosideng.m.tmall.com',\n",
       " 'chowtaifook.m.tmall.com',\n",
       " 'goelia.m.tmall.com',\n",
       " 'kipone.m.tmall.com',\n",
       " 'osa.m.tmall.com',\n",
       " 'gxg.m.tmall.com',\n",
       " 'basichouse.m.tmall.com',\n",
       " 'liby.tmall.com',\n",
       " 'davebella.m.tmall.com',\n",
       " 'microsoftstore.m.tmall.com',\n",
       " 'yalu.m.tmall.com',\n",
       " 'gap.tmall.com',\n",
       " 'nestle.m.tmall.com',\n",
       " 'microsoftstore.tmall.com',\n",
       " 'tmcc.tmall.com',\n",
       " 'meadjohnson.m.tmall.com',\n",
       " 'dasandeyu.m.tmall.com',\n",
       " 'kmax.m.tmall.com',\n",
       " 'elegantprosper.tmall.com',\n",
       " 'haibangmy.m.tmall.com',\n",
       " 'xiaomi.m.tmall.com',\n",
       " 'bejirog.m.tmall.com',\n",
       " 'samsung.m.tmall.com',\n",
       " 'tanmujiang.m.tmall.com',\n",
       " 'selected.m.tmall.com',\n",
       " 'qianzhihe.m.tmall.com',\n",
       " 'lianhelihua.m.tmall.com',\n",
       " 'fairwhale.m.tmall.com',\n",
       " 'laopai.m.tmall.com',\n",
       " 'winshare.tmall.com',\n",
       " 'ukyo.m.tmall.com',\n",
       " 'ripfs.tmall.com',\n",
       " 'tianxiangsh.m.tmall.com',\n",
       " 'yearcon.m.tmall.com',\n",
       " 'ruxi.m.tmall.com',\n",
       " 'ygcp.m.tmall.com',\n",
       " 'olay.m.tmall.com',\n",
       " 'newbalance-tmall.com',\n",
       " 'pwts.m.tmall.com',\n",
       " '3m.m.tmall.com',\n",
       " 'yunifang.m.tmall.com',\n",
       " 'hotwind.m.tmall.com',\n",
       " 'ladyangel.m.tmall.com',\n",
       " 'converse.m.tmall.com',\n",
       " 'tcldq.m.tmall.com',\n",
       " 'cityliferx.m.tmall.com',\n",
       " 'dixism.m.tmall.com',\n",
       " 'sdeer.m.tmall.com',\n",
       " 'joeone.m.tmall.com',\n",
       " 'kangximuye.m.tmall.com',\n",
       " 'yobeyi.m.tmall.com',\n",
       " 'jackjones.tmall.com',\n",
       " 'hongyuanxin.tmall.com',\n",
       " 'laopai.tmall.com',\n",
       " 'epson.tmall.com',\n",
       " 'chikoolmt.m.tmall.com',\n",
       " 'othermix.m.tmall.com',\n",
       " 'toread.m.tmall.com',\n",
       " 'northland.m.tmall.com',\n",
       " 'xinjumingjia.m.tmall.com',\n",
       " 'suxingdeleyuan.m.tmall.com',\n",
       " 'www.tmall-cnas.com',\n",
       " 'ganso.m.tmall.com',\n",
       " 'eskin.m.tmall.com',\n",
       " 'zippo.tmall.com',\n",
       " 'miaoxin.tmall.com',\n",
       " 'shitingluyaomn.m.tmall.com',\n",
       " 'anzhufs.m.tmall.com',\n",
       " 'kakashizixiu.m.tmall.com',\n",
       " 'tcldq.tmall.com',\n",
       " 'mb.m.tmall.com',\n",
       " '9i.m.tmall.com',\n",
       " 'belle.m.tmall.com',\n",
       " 'rocksm.m.tmall.com',\n",
       " 'vattizm.tmall.com',\n",
       " 'lagogo.m.tmall.com',\n",
       " 'img.tmallcdn.com',\n",
       " 'heinz.m.tmall.com',\n",
       " 'asusbjb.m.tmall.com',\n",
       " 'tangchenbeijian.m.tmall.com',\n",
       " 'yiyexing.m.tmall.com',\n",
       " 'kstjj.m.tmall.com',\n",
       " 'angelcitiz.m.tmall.com',\n",
       " 'triph5.m.tmall.com',\n",
       " 'oppo.m.tmall.com',\n",
       " 'ashimabjh.m.tmall.com',\n",
       " 'nmsm.m.tmall.com',\n",
       " 'mdqsdq.m.tmall.com',\n",
       " 'epson.m.tmall.com',\n",
       " 'eral.m.tmall.com',\n",
       " 'mfsj1908.m.tmall.com',\n",
       " 'loreal.tmall.com',\n",
       " 'viscap.m.tmall.com',\n",
       " 'koukoufu.m.tmall.com',\n",
       " 'chowsangsang.m.tmall.com',\n",
       " 'playboyny.m.tmall.com',\n",
       " 'runyiyifs.m.tmall.com',\n",
       " 'tmallsns.taobao.com',\n",
       " 'meizu.m.tmall.com',\n",
       " 'loveesteem.m.tmall.com',\n",
       " 'lshmy.tmall.com',\n",
       " 'lafaso.m.tmall.com',\n",
       " 'www.canadagoose-tmall.com',\n",
       " 'kbird.m.tmall.com',\n",
       " 'nollmet.m.tmall.com',\n",
       " 'junyujj.m.tmall.com',\n",
       " 'baiyiya.m.tmall.com',\n",
       " 'skii.m.tmall.com',\n",
       " 'gurunvanifs.m.tmall.com',\n",
       " 'mohanyimei.m.tmall.com',\n",
       " 'sdeerconcept.m.tmall.com',\n",
       " 'hodohome.m.tmall.com',\n",
       " 'shsiemens.m.tmall.com',\n",
       " 'pb89.m.tmall.com',\n",
       " 'cachecachebmnw.m.tmall.com',\n",
       " 'esprit.m.tmall.com',\n",
       " 'triplenineny.m.tmall.com',\n",
       " 'umdnj.m.tmall.com',\n",
       " 'wuzhoushipin.m.tmall.com',\n",
       " 'jealousy.m.tmall.com',\n",
       " 'camelsports.m.tmall.com',\n",
       " 'jonaswagell.m.tmall.com',\n",
       " 'achette.m.tmall.com',\n",
       " 'feishengmy.m.tmall.com',\n",
       " 'huawei.m.tmall.com',\n",
       " 'nautica.m.tmall.com',\n",
       " 'deesha.m.tmall.com',\n",
       " 'ziwei.m.tmall.com',\n",
       " 'richricas.m.tmall.com',\n",
       " 'lzfur.m.tmall.com',\n",
       " 'ssbsm.m.tmall.com',\n",
       " 'isido.m.tmall.com',\n",
       " 'fanersen.m.tmall.com',\n",
       " 'kaqiwu.m.tmall.com',\n",
       " 'metersbonwe.tmall.com',\n",
       " 'xychjj.m.tmall.com',\n",
       " 'chjjewellery.m.tmall.com',\n",
       " 'romon.tmall.com',\n",
       " 'kaifandiml.m.tmall.com',\n",
       " 'mumuhome.m.tmall.com',\n",
       " 'amh.m.tmall.com',\n",
       " 'omifs.m.tmall.com',\n",
       " 'easterncamel.m.tmall.com',\n",
       " 'brita.m.tmall.com',\n",
       " 'kao.m.tmall.com',\n",
       " 'yearconnx.m.tmall.com',\n",
       " 'htshuma.m.tmall.com',\n",
       " 'fmart.tmall.com',\n",
       " 'tdwfs.m.tmall.com',\n",
       " 'balenojd.tmall.com',\n",
       " 'lenovo.m.tmall.com',\n",
       " 'a02.m.tmall.com',\n",
       " 'afwj.m.tmall.com',\n",
       " 'nuoleidun.m.tmall.com',\n",
       " 'xiaotao.m.tmall.com',\n",
       " 'qiyaqcyp.m.tmall.com',\n",
       " 'yuzhaolinzj.m.tmall.com',\n",
       " 'ebcfs.m.tmall.com',\n",
       " 'angrybirds.tmall.com',\n",
       " 'clioborn.tmall.com',\n",
       " 'honb.m.tmall.com',\n",
       " 'ansels.m.tmall.com',\n",
       " 'www.jackwolfskin-tmall.com',\n",
       " 'nanjiren.m.tmall.com',\n",
       " 'ugiz.m.tmall.com',\n",
       " 'lingwuqiyishijia.m.tmall.com',\n",
       " 'yiwensp.m.tmall.com',\n",
       " 'zuoluoxiansheng.tmall.com',\n",
       " 'yixiangliying.m.tmall.com',\n",
       " 'antakids.tmall.com',\n",
       " 'fuanna.m.tmall.com',\n",
       " 'artka.tmall.com',\n",
       " 'angrybirds.m.tmall.com',\n",
       " 'jplus.m.tmall.com',\n",
       " 'nbastore.m.tmall.com',\n",
       " 'hqyt.m.tmall.com',\n",
       " 'lailaimy.m.tmall.com',\n",
       " 'shunvfang.m.tmall.com',\n",
       " 'dphone.m.tmall.com',\n",
       " 'forge.m.tmall.com',\n",
       " 'snowforte.m.tmall.com',\n",
       " 'nanjirenfh.m.tmall.com',\n",
       " 'mikibana.m.tmall.com',\n",
       " 'fensejisql.m.tmall.com',\n",
       " 'jiazhuli.m.tmall.com',\n",
       " 'mxm.m.tmall.com',\n",
       " 'liby.m.tmall.com',\n",
       " 'beibeixiong.m.tmall.com',\n",
       " 'fanqi.m.tmall.com',\n",
       " 'adoodoo.m.tmall.com',\n",
       " 'xlxsp.m.tmall.com',\n",
       " 'toplanyde.m.tmall.com',\n",
       " 'gebaosz.m.tmall.com',\n",
       " 'hems.m.tmall.com',\n",
       " 'ka.tmall.com',\n",
       " 'httz.m.tmall.com',\n",
       " 'honb.tmall.com',\n",
       " 'jasonwood.m.tmall.com',\n",
       " 'sankins.m.tmall.com',\n",
       " 'watsons.tmall.com',\n",
       " 'zbwj.m.tmall.com',\n",
       " 'donoraticogf.m.tmall.com',\n",
       " 'xinranjj.m.tmall.com',\n",
       " 'brita.tmall.com',\n",
       " 'bkyd.m.tmall.com',\n",
       " 'senda.m.tmall.com',\n",
       " 'nanjirenyz.m.tmall.com',\n",
       " 'afsjeep.m.tmall.com',\n",
       " 'bejirogmy.tmall.com',\n",
       " 'ojays.m.tmall.com',\n",
       " 'yijialun.m.tmall.com',\n",
       " 'waayes.m.tmall.com',\n",
       " 'dabuwawa.m.tmall.com',\n",
       " 'ygcp.tmall.com',\n",
       " 'gradyboy.m.tmall.com',\n",
       " 'dingdangxiaozhu.m.tmall.com',\n",
       " 'lego.m.tmall.com',\n",
       " 'suyujj.m.tmall.com',\n",
       " 'aimiss.m.tmall.com',\n",
       " 'powerdekor.tmall.com',\n",
       " 'queend.m.tmall.com',\n",
       " 'hasbro.m.tmall.com',\n",
       " 'hezhifs.m.tmall.com',\n",
       " '27ri.m.tmall.com',\n",
       " 'elle.m.tmall.com',\n",
       " 'cmzx.m.tmall.com',\n",
       " 'leistress.m.tmall.com',\n",
       " 'fmart.m.tmall.com',\n",
       " 'luys.m.tmall.com',\n",
       " 'xinchaojj.tmall.com',\n",
       " 'dove.m.tmall.com',\n",
       " 'uniqlo.tmall.com',\n",
       " 'yidunjide.m.tmall.com',\n",
       " 'baomanfs.m.tmall.com',\n",
       " 'qiantx.m.tmall.com',\n",
       " 'toread.tmall.com',\n",
       " 'czcbsts.m.tmall.com',\n",
       " 'turnsignal.m.tmall.com',\n",
       " 'fgsm.m.tmall.com',\n",
       " 'suppod.m.tmall.com',\n",
       " 'zhenchuanfs.m.tmall.com',\n",
       " 'uniwal.tmall.com',\n",
       " 'ymygfs.m.tmall.com',\n",
       " 'xrui.m.tmall.com',\n",
       " 'staccato.m.tmall.com',\n",
       " 'jfxysp.m.tmall.com',\n",
       " 'neutrogena.m.tmall.com',\n",
       " 'mexicannb.m.tmall.com',\n",
       " '1828nanzhuang.m.tmall.com',\n",
       " 'erke.m.tmall.com',\n",
       " 'lvsensm.m.tmall.com',\n",
       " 'rmmfs.m.tmall.com',\n",
       " 'lsspring.m.tmall.com',\n",
       " 'songzhuts.m.tmall.com',\n",
       " 'busen.m.tmall.com',\n",
       " 'vivala.m.tmall.com',\n",
       " 'xxcwyp.m.tmall.com',\n",
       " 'malilan.m.tmall.com',\n",
       " 'manhua.m.tmall.com',\n",
       " 'eoshzp.tmall.com',\n",
       " 'beely.m.tmall.com',\n",
       " 'cbanner.m.tmall.com',\n",
       " 'suuntohwmj.m.tmall.com',\n",
       " 'moen.m.tmall.com',\n",
       " 'eminencestoney.m.tmall.com',\n",
       " 'yiluodi.m.tmall.com',\n",
       " 'eifini.m.tmall.com',\n",
       " 'bkny.m.tmall.com',\n",
       " 'kufei.m.tmall.com',\n",
       " 'wandian.m.tmall.com',\n",
       " 'haier.m.tmall.com',\n",
       " 'bosidengnyy.m.tmall.com',\n",
       " 'mojay.m.tmall.com',\n",
       " 'lxdxb.m.tmall.com',\n",
       " 'tmall.com',\n",
       " 'peacebird.m.tmall.com',\n",
       " 'maxfactor.m.tmall.com',\n",
       " 'hangyige.m.tmall.com',\n",
       " 'fuguixiong.m.tmall.com',\n",
       " 'shehexw.tmall.com',\n",
       " 'yanazi.m.tmall.com',\n",
       " 'qinghuayang.m.tmall.com',\n",
       " 'wyqcyp.m.tmall.com',\n",
       " 'qjzm.tmall.com',\n",
       " 'xiaoxiaohai.m.tmall.com',\n",
       " 'ltnx.m.tmall.com',\n",
       " 'lmfs.m.tmall.com',\n",
       " 'rrjjry.tmall.com',\n",
       " 'qxnwhf.m.tmall.com',\n",
       " 'delivery.tmall.com',\n",
       " 'gtjj.m.tmall.com',\n",
       " 'eland.m.tmall.com',\n",
       " 'siyake.m.tmall.com',\n",
       " 'bonjung.m.tmall.com',\n",
       " 'haoxiangni.m.tmall.com',\n",
       " 'hems.tmall.com',\n",
       " 'supfiresh.m.tmall.com',\n",
       " 'ruiao.tmall.com',\n",
       " 'okko.m.tmall.com',\n",
       " 'kipone.tmall.com',\n",
       " 'xinnengliang.m.tmall.com',\n",
       " 'syoss.m.tmall.com',\n",
       " 'yifengfs.m.tmall.com',\n",
       " 'cofco.m.tmall.com',\n",
       " 'saintgirl.m.tmall.com',\n",
       " 'baicaowei.m.tmall.com',\n",
       " 'qilekang.tmall.com',\n",
       " 'nicilq.m.tmall.com',\n",
       " 'yytsm.tmall.com',\n",
       " 'jindasheng.m.tmall.com',\n",
       " 'joyoungtp.m.tmall.com',\n",
       " 'adidas.tmall.com',\n",
       " 'thermos.m.tmall.com',\n",
       " 'qiaosha.m.tmall.com',\n",
       " 'pisenhzj.m.tmall.com',\n",
       " 'lwxl.m.tmall.com',\n",
       " 'carenzdz.m.tmall.com',\n",
       " 'gainreel.m.tmall.com',\n",
       " 'yihaodian.m.tmall.com',\n",
       " 'xicunmingwu.m.tmall.com',\n",
       " 'lotionspa.m.tmall.com',\n",
       " 'hzxs.m.tmall.com',\n",
       " 'zhijinlou.m.tmall.com',\n",
       " 'juxingyihaosrt.m.tmall.com',\n",
       " 'liangzuxl.m.tmall.com',\n",
       " 'junlinyd.m.tmall.com',\n",
       " 'goodbaby.m.tmall.com',\n",
       " 'aiken.m.tmall.com',\n",
       " 'zhenbeihz.m.tmall.com']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in model.index2word  if \"tmall\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count----> 100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'media_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-75706e1fe207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count---->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedia_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mshop_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmap_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'media_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "for count in [100,200,300,400,500,700,800]:\n",
    "    print(\"count---->\",count)\n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42)\n",
    "    clf_SVM = SVM.fit(x_train, y_train)\n",
    "    doc_class_predicted = clf_SVM.predict(x_test)\n",
    "    print(np.mean(doc_class_predicted == y_test))\n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count----> 100\n",
      "62 0\n",
      "99 0\n",
      "83 0\n",
      "0.8133333333333334\n",
      "count----> 200\n",
      "187 2\n",
      "189 2\n",
      "107 1\n",
      "0.805\n",
      "count----> 300\n",
      "172 0\n",
      "297 0\n",
      "188 2\n",
      "0.73\n",
      "count----> 400\n",
      "379 1\n",
      "398 1\n",
      "172 1\n",
      "0.7908333333333334\n",
      "count----> 500\n",
      "382 0\n",
      "444 0\n",
      "241 0\n",
      "0.7113333333333334\n",
      "count----> 700\n",
      "563 0\n",
      "472 0\n",
      "131 0\n",
      "0.5552380952380952\n",
      "count----> 800\n",
      "761 1\n",
      "510 1\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "no mode for empty data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-2fc57f856835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/statistics.py\u001b[0m in \u001b[0;36mmode\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 )\n\u001b[1;32m    476\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no mode for empty data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStatisticsError\u001b[0m: no mode for empty data"
     ]
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "for count in [100,200,300,400,500,700,800]:\n",
    "    print(\"count---->\",count)\n",
    "    final_embeddings= media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "    len(final_embeddings)\n",
    "    from sklearn import cluster, datasets\n",
    "    X = final_embeddings\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "#     print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ),mode(k_means.labels_[:count:]))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]),mode(k_means.labels_[count:count+count:]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]),mode(k_means.labels_[count+count:count+count+count:]))\n",
    "\n",
    "    print(right_count/(3*count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count----> 100\n",
      "0.94\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.93      0.93        54\n",
      "          1       0.93      0.95      0.94        44\n",
      "          2       0.94      0.94      0.94        52\n",
      "\n",
      "avg / total       0.94      0.94      0.94       150\n",
      "\n",
      "count----> 200\n",
      "0.956666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95        97\n",
      "          1       0.99      0.93      0.96       107\n",
      "          2       0.94      0.97      0.95        96\n",
      "\n",
      "avg / total       0.96      0.96      0.96       300\n",
      "\n",
      "count----> 300\n",
      "0.926666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.93       154\n",
      "          1       0.91      0.97      0.94       153\n",
      "          2       0.94      0.87      0.91       143\n",
      "\n",
      "avg / total       0.93      0.93      0.93       450\n",
      "\n",
      "count----> 400\n",
      "0.93\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93       204\n",
      "          1       0.92      0.98      0.95       198\n",
      "          2       0.95      0.88      0.92       198\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "count----> 500\n",
      "0.918666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.94      0.93       263\n",
      "          1       0.87      0.95      0.91       239\n",
      "          2       0.97      0.87      0.92       248\n",
      "\n",
      "avg / total       0.92      0.92      0.92       750\n",
      "\n",
      "count----> 700\n",
      "0.920952380952\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.93      0.92       346\n",
      "          1       0.90      0.92      0.91       357\n",
      "          2       0.97      0.91      0.94       347\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1050\n",
      "\n",
      "count----> 800\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [2341 2400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-c804cb3e66a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [2341 2400]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for count in [100,200,300,400,500,700,800]:\n",
    "    print(\"count---->\",count)\n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    clf_SVM = SVM.fit(x_train, y_train)\n",
    "    doc_class_predicted = clf_SVM.predict(x_test)\n",
    "    print(np.mean(doc_class_predicted == y_test))\n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88013601,  0.849029  ,  2.06500793,  1.47481406,  1.05131602,\n",
       "       -3.23556709, -1.422328  , -0.66254503, -2.1743691 , -1.54976594,\n",
       "        2.05652595,  0.91469598,  2.60806298, -1.54639602, -1.68464506,\n",
       "       -2.83229899, -0.90186101, -1.67103899, -2.12337589,  0.98171002,\n",
       "        1.29474699,  1.37360597, -2.80476189,  0.908171  ,  1.66404498,\n",
       "        0.87784398, -0.29884499,  0.422692  ,  1.24100804,  4.40271807,\n",
       "       -1.06018102, -1.56210601, -2.58844805, -1.67301202,  3.1986351 ,\n",
       "        0.53300899,  0.188034  , -1.94363403,  0.81556201, -0.3985    ,\n",
       "        1.75758898,  1.45460296, -1.87927902, -1.28549194, -1.89206004,\n",
       "       -1.11207497,  0.43751499,  0.82267702,  1.56893802, -0.16922501,\n",
       "        2.41652894, -0.19342899,  0.97147399,  0.37545401, -1.00442803,\n",
       "        0.235394  , -2.95109606,  1.45366204, -1.11558998,  1.81515205,\n",
       "       -0.06049   ,  1.00733197,  3.85380006, -1.06283998, -0.92553002,\n",
       "        1.78450596, -1.25091398,  0.53140301, -1.37926805, -0.25111601,\n",
       "        0.90266103, -1.23709297,  1.61233497, -0.49246201,  2.79231405,\n",
       "        0.19640701, -0.090707  , -0.39385599, -1.477525  , -1.30097604,\n",
       "        0.200883  , -0.95764601,  2.01641202, -0.18790901,  1.44120705,\n",
       "       -1.52755296,  0.238389  ,  0.261163  ,  1.90750802, -0.84829497,\n",
       "       -0.106365  ,  1.738397  ,  1.38577199, -0.22723299,  0.352063  ,\n",
       "       -0.183576  , -0.89239699, -0.74943399,  0.238685  , -0.99775201], dtype=float32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "clf_SVM = SVM.fit(x_train, y_train)\n",
    "doc_class_predicted = clf_SVM.predict(x_test)\n",
    "print(np.mean(doc_class_predicted == y_test))\n",
    "print(metrics.classification_report(y_test, doc_class_predicted))\n",
    "    \n",
    "for i in range(10):\n",
    "    print(clf_SVM.predict(aliyun_embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose the best windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size1\n",
      "600\n",
      "197\n",
      "183\n",
      "155\n",
      "1 ------>>>> 0.8916666666666667 \n",
      "\n",
      "train-model-size2\n",
      "600\n",
      "187\n",
      "192\n",
      "173\n",
      "2 ------>>>> 0.92 \n",
      "\n",
      "train-model-size3\n",
      "600\n",
      "179\n",
      "189\n",
      "164\n",
      "3 ------>>>> 0.8866666666666667 \n",
      "\n",
      "train-model-size5\n",
      "600\n",
      "182\n",
      "193\n",
      "143\n",
      "5 ------>>>> 0.8633333333333333 \n",
      "\n",
      "train-model-size7\n",
      "600\n",
      "175\n",
      "194\n",
      "106\n",
      "7 ------>>>> 0.7916666666666666 \n",
      "\n",
      "train-model-size10\n",
      "600\n",
      "174\n",
      "196\n",
      "144\n",
      "10 ------>>>> 0.8566666666666667 \n",
      "\n",
      "train-model-size13\n",
      "600\n",
      "184\n",
      "193\n",
      "170\n",
      "13 ------>>>> 0.9116666666666666 \n",
      "\n",
      "train-model-size15\n",
      "600\n",
      "177\n",
      "196\n",
      "168\n",
      "15 ------>>>> 0.9016666666666666 \n",
      "\n",
      "train-model-size20\n",
      "600\n",
      "145\n",
      "195\n",
      "173\n",
      "20 ------>>>> 0.855 \n",
      "\n",
      "train-model-size30\n",
      "600\n",
      "133\n",
      "197\n",
      "141\n",
      "30 ------>>>> 0.785 \n",
      "\n",
      "train-model-size50\n",
      "600\n",
      "142\n",
      "191\n",
      "162\n",
      "50 ------>>>> 0.825 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [1,2,3,5,7,10,13,15,20,30,50]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn import cluster, datasets\n",
    "    from statistics import mode\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "    print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "    print(i,\"------>>>>\",right_count/(3*count),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose the best model-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n",
      "600\n",
      "120\n",
      "142\n",
      "135\n",
      "2 ------>>>> 0.6616666666666666 \n",
      "\n",
      "train-model-size5\n",
      "600\n",
      "169\n",
      "196\n",
      "193\n",
      "5 ------>>>> 0.93 \n",
      "\n",
      "train-model-size10\n",
      "600\n",
      "195\n",
      "199\n",
      "187\n",
      "10 ------>>>> 0.9683333333333334 \n",
      "\n",
      "train-model-size15\n",
      "600\n",
      "192\n",
      "198\n",
      "185\n",
      "15 ------>>>> 0.9583333333333334 \n",
      "\n",
      "train-model-size20\n",
      "600\n",
      "191\n",
      "199\n",
      "179\n",
      "20 ------>>>> 0.9483333333333334 \n",
      "\n",
      "train-model-size50\n",
      "600\n",
      "182\n",
      "195\n",
      "139\n",
      "50 ------>>>> 0.86 \n",
      "\n",
      "train-model-size80\n",
      "600\n",
      "179\n",
      "192\n",
      "110\n",
      "80 ------>>>> 0.8016666666666666 \n",
      "\n",
      "train-model-size100\n",
      "600\n",
      "176\n",
      "195\n",
      "94\n",
      "100 ------>>>> 0.775 \n",
      "\n",
      "train-model-size200\n",
      "600\n",
      "177\n",
      "189\n",
      "84\n",
      "200 ------>>>> 0.75 \n",
      "\n",
      "train-model-size500\n",
      "600\n",
      "165\n",
      "188\n",
      "94\n",
      "500 ------>>>> 0.745 \n",
      "\n",
      "train-model-size1000\n",
      "600\n",
      "157\n",
      "194\n",
      "89\n",
      "1000 ------>>>> 0.7333333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn import cluster, datasets\n",
    "    from statistics import mode\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "    print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "    print(i,\"------>>>>\",right_count/(3*count),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n",
      "600\n",
      "87\n",
      "101\n",
      "97\n",
      "2 ------>>>> 0.475 \n",
      "\n",
      "train-model-size5\n",
      "600\n",
      "173\n",
      "196\n",
      "190\n",
      "5 ------>>>> 0.9316666666666666 \n",
      "\n",
      "train-model-size10\n",
      "600\n",
      "192\n",
      "195\n",
      "168\n",
      "10 ------>>>> 0.925 \n",
      "\n",
      "train-model-size15\n",
      "600\n",
      "197\n",
      "198\n",
      "170\n",
      "15 ------>>>> 0.9416666666666667 \n",
      "\n",
      "train-model-size20\n",
      "600\n",
      "193\n",
      "197\n",
      "171\n",
      "20 ------>>>> 0.935 \n",
      "\n",
      "train-model-size50\n",
      "600\n",
      "189\n",
      "193\n",
      "169\n",
      "50 ------>>>> 0.9183333333333333 \n",
      "\n",
      "train-model-size80\n",
      "600\n",
      "184\n",
      "193\n",
      "138\n",
      "80 ------>>>> 0.8583333333333333 \n",
      "\n",
      "train-model-size100\n",
      "600\n",
      "188\n",
      "191\n",
      "174\n",
      "100 ------>>>> 0.9216666666666666 \n",
      "\n",
      "train-model-size200\n",
      "600\n",
      "184\n",
      "186\n",
      "89\n",
      "200 ------>>>> 0.765 \n",
      "\n",
      "train-model-size500\n",
      "600\n",
      "162\n",
      "163\n",
      "188\n",
      "500 ------>>>> 0.855 \n",
      "\n",
      "train-model-size1000\n",
      "600\n",
      "189\n",
      "161\n",
      "104\n",
      "1000 ------>>>> 0.7566666666666667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn import cluster, datasets\n",
    "    from statistics import mode\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "    print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "    print(i,\"------>>>>\",right_count/(3*count),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n",
      "600\n",
      "132\n",
      "179\n",
      "102\n",
      "2 ------>>>> 0.6883333333333334 \n",
      "\n",
      "train-model-size5\n",
      "600\n",
      "127\n",
      "191\n",
      "115\n",
      "5 ------>>>> 0.7216666666666667 \n",
      "\n",
      "train-model-size10\n",
      "600\n",
      "149\n",
      "195\n",
      "80\n",
      "10 ------>>>> 0.7066666666666667 \n",
      "\n",
      "train-model-size15\n",
      "600\n",
      "143\n",
      "198\n",
      "104\n",
      "15 ------>>>> 0.7416666666666667 \n",
      "\n",
      "train-model-size20\n",
      "600\n",
      "146\n",
      "198\n",
      "97\n",
      "20 ------>>>> 0.735 \n",
      "\n",
      "train-model-size50\n",
      "600\n",
      "189\n",
      "191\n",
      "114\n",
      "50 ------>>>> 0.8233333333333334 \n",
      "\n",
      "train-model-size80\n",
      "600\n",
      "115\n",
      "199\n",
      "109\n",
      "80 ------>>>> 0.705 \n",
      "\n",
      "train-model-size100\n",
      "600\n",
      "120\n",
      "197\n",
      "150\n",
      "100 ------>>>> 0.7783333333333333 \n",
      "\n",
      "train-model-size200\n",
      "600\n",
      "85\n",
      "187\n",
      "88\n",
      "200 ------>>>> 0.6 \n",
      "\n",
      "train-model-size500\n",
      "600\n",
      "112\n",
      "200\n",
      "145\n",
      "500 ------>>>> 0.7616666666666667 \n",
      "\n",
      "train-model-size1000\n",
      "600\n",
      "190\n",
      "200\n",
      "178\n",
      "1000 ------>>>> 0.9466666666666667 \n",
      "\n",
      "train-model-size2000\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000,2000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "    map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "    media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn import cluster, datasets\n",
    "    from statistics import mode\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "    print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "    print(i,\"------>>>>\",right_count/(3*count),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    \n",
    "    shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "    map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "    media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn import cluster, datasets\n",
    "    from statistics import mode\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "    print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "    print(i,\"------>>>>\",right_count/(3*count),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 1 2 0 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 2 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1\n",
      " 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 2 1 0 1 0 0 1 1 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 2 2 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
      " 1 0 2 1 1 2 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 2 1 1 1 1 2 1 2 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 2 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 2 1 1 0 1 0 0 1 1 1\n",
      " 1 1 1 2 1 1 0 1]\n",
      "600\n",
      "185\n",
      "196\n",
      "138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "count = 200\n",
    "final_embeddings=baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "len(final_embeddings)\n",
    "from sklearn import cluster, datasets\n",
    "X = final_embeddings\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X)\n",
    "print(k_means.labels_[::])\n",
    "print(len(k_means.labels_[::]))\n",
    "\n",
    "right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "right_count/(3*count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 1 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 2 2 3 3 1\n",
      " 3 3 3 3 3 1 3 3 1 3 3 3 3 3 3 0 3 3 3 1 3 3 3 3 3 3 2 3 3 3 3 1 1 3 0 3 3\n",
      " 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2\n",
      " 3 1 2 3 3 2 3 2 3 1 3 3 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 1 1 2 2 2 1 2 2 3 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1\n",
      " 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 3 2 2 1 2 2 2 1 2 2 2 2 0 2 1 1 3 2 3 1 1 2 2 1 2 2 2 2 1 2 1 1\n",
      " 1 3 2 2 2 1 1 2 2 2 2 2 2 2 3 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 1 2 2 2 2 2 2 1 2 2 2 2 1 1 1 2 2 1 1 1 2 2 2 1 2 1 2 2 1 1 2 2 2 1 2 2 1\n",
      " 1 2 2 2 2 1 2 1 1 2 2 2 2 2 2 1 2 1 2 1 2 1 2 2 3 1 2 2 2 2 1 2 2 2 1 2 2\n",
      " 2 2 2 0 2 3 2 2 2 3 2 1 1 1 3 3 1 3 2 1 2 2 2 2 1 2 2 3 2 2 3 2 2 2 3 1 2\n",
      " 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 0 1 0 1 0\n",
      " 1 3 0 1 2 2 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 2 1 1 2 1 1 1 1 2 1 2 1 1 1 2\n",
      " 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 0 1 1 1 1 2 1 1 1 1 1 3 0 1 1 1 1 1 3 0 1\n",
      " 1 1 2 3 1 0 1 2 0 1 2 2 2 1 1 1 1 1 1 1 2 0 1 3 2 3 1 1 2 2 1 1 1 1 2 2 1\n",
      " 3 1 2 3 2 2 2 0 1 1 1 1 1 1 1 2 2 0 1 0 1 0 2 3 1 1 2 2 1 2 1 2 0 3 1 2 1\n",
      " 1 3 2 3 2 1 1 1 2 1 3 3 2 2 1 1 2 3 3 1 2 1 1 0 2 0 2 0 0 0 2 0 2 0 0 2 0\n",
      " 2 0 2 0 0 1 0 0 2 2 1 2 0 0 2 0 0 3 0 0 0 0 0 0 0 0 0 2 0 0 2 0 1 0 0 0 0\n",
      " 2 0 0 0 0 0 2 0 1 0 3 0 3 1 0 0 0 1 0 1 0 0 0 0 0 2 0 0 0 0 0 0 3 2 0 3 0\n",
      " 0 0 0 0 1 0 0 0 0 2 0 0 0 0 2 1 1 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0\n",
      " 0 2 0 0 0 0 0 0 0 2 0 0 0 0 1 0 2 0 2 0 2 1 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0\n",
      " 2 1 0 0 2 0 0 0 0 2 0 2 0 0 2 2 0 2 0 1 1 0 0 2 0 2 0 0 0 0 0 0 0 3 0 0 0\n",
      " 0]\n",
      "1000\n",
      "619\n",
      "172\n",
      "187\n",
      "136\n",
      "124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "count = 200\n",
    "final_embeddings=baidu_embeddings[:count]+taobao_embeddings[:count]+sina_embeddings[:count]+uc_embeddings[:count]+qq_embeddings[:count]\n",
    "len(final_embeddings)\n",
    "from sklearn import cluster, datasets\n",
    "X = final_embeddings\n",
    "k_means = cluster.KMeans(n_clusters=4)\n",
    "k_means.fit(X)\n",
    "print(k_means.labels_[::])\n",
    "print(len(k_means.labels_[::]))\n",
    "\n",
    "right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) + len([i for i in k_means.labels_[count+count+count:count+count+count+count:] if i==mode(k_means.labels_[count+count+count:count+count+count+count:])]) \n",
    "\n",
    "print(right_count)\n",
    "print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "print(len([i for i in k_means.labels_[count+count+count:count+count+count+count:] if i==mode(k_means.labels_[count+count+count:count+count+count+count:])]) )\n",
    "\n",
    "right_count/(4*count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode(k_means.labels_[:count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('update.xyq.163.com', 0.6822214126586914),\n",
       " ('res.xyq.cbg.163.com', 0.6381403207778931),\n",
       " ('123.58.170.12', 0.6181110143661499),\n",
       " ('xy2-android.cbg.163.com', 0.5335538387298584),\n",
       " ('xyq-ios.cbg.163.com', 0.5245332717895508),\n",
       " ('183.61.2.94', 0.5020437240600586),\n",
       " ('service.mkey.163.com', 0.4834635853767395),\n",
       " ('123.58.173.180', 0.47402223944664),\n",
       " ('lbs.dd.163.com', 0.46957576274871826),\n",
       " ('res.nie.netease.com', 0.46432632207870483)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"xyq-android.cbg.163.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cn.patch.battle.net', 0.9757399559020996),\n",
       " ('client02.pdl.wow.battlenet.com.cn', 0.9654581546783447),\n",
       " ('dist.blizzard.com.edgesuite.net', 0.9624665975570679),\n",
       " ('zhcn.patch.battle.net', 0.9200407266616821),\n",
       " ('cn.launcher.battlenet.com.cn', 0.9119179248809814),\n",
       " ('cn-test.patch.battle.net', 0.8966389298439026),\n",
       " ('public-test.patch.battle.net', 0.8699407577514648),\n",
       " ('client01.pdl.wow.battlenet.com.cn', 0.8346480131149292),\n",
       " ('template.wps.cn', 0.8248226642608643),\n",
       " ('client03.pdl.wow.battlenet.com.cn', 0.8204830884933472)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"iir.blizzard.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client02.pdl.wow.battlenet.com.cn', 0.978783130645752),\n",
       " ('iir.blizzard.com', 0.9757399559020996),\n",
       " ('dist.blizzard.com.edgesuite.net', 0.9612374901771545),\n",
       " ('zhcn.patch.battle.net', 0.9320716261863708),\n",
       " ('cn.launcher.battlenet.com.cn', 0.9257140159606934),\n",
       " ('cn-test.patch.battle.net', 0.9091231822967529),\n",
       " ('public-test.patch.battle.net', 0.8897701501846313),\n",
       " ('client01.pdl.wow.battlenet.com.cn', 0.8683668375015259),\n",
       " ('client03.pdl.wow.battlenet.com.cn', 0.8408011198043823),\n",
       " ('template.wps.cn', 0.7996641397476196)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"cn.patch.battle.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"client02.pdl.wow.battlenet.com.cn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"pdl\" in a  or \"02\"  in a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
