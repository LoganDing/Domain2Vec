{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size100\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#for i in [100,200,500,1000,2000]:\n",
    "for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "uc_embeddings = [model[word] for word in model.index2word  if \"uc\" in word]\n",
    "umeng_embeddings = [model[word] for word in model.index2word  if \"umeng\" in word]\n",
    "sina_embeddings = [model[word] for word in model.index2word  if \"sina\" in word]\n",
    "taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "qiyi_embeddings = [model[word] for word in model.index2word  if \"qiyi\" in word]\n",
    "wangyi_embeddings = [model[word] for word in model.index2word  if \"163.\" in word]\n",
    "video_embeddings = [model[word] for word in model.index2word  if \"video\" in word]\n",
    "music_embeddings = [model[word] for word in model.index2word  if \"music\" in word]\n",
    "game_embeddings = [model[word] for word in model.index2word  if \"game\" in word]\n",
    "news_embeddings = [model[word] for word in model.index2word  if \"news\" in word]\n",
    "book_embeddings = [model[word] for word in model.index2word  if \"book\" in word]\n",
    "apps_embeddings = [model[word] for word in model.index2word  if \"apps\" in word]\n",
    "img_embeddings = [model[word] for word in model.index2word  if \"img\" in word]\n",
    "shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "pay_embeddings = [model[word] for word in model.index2word  if \"pay\" in word]\n",
    "disk_embeddings = [model[word] for word in model.index2word  if \"disk\" in word]\n",
    "yun_embeddings = [model[word] for word in model.index2word  if \"yun\" in word]\n",
    "map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "meituan_embeddings = [model[word] for word in model.index2word  if \"meituan\" in word]\n",
    "weibo_embeddings = [model[word] for word in model.index2word  if \"weibo\" in word]\n",
    "chat_embeddings = [model[word] for word in model.index2word  if \"chat\" in word]\n",
    "download_embeddings = [model[word] for word in model.index2word  if \"download\" in word]\n",
    "api_embeddings = [model[word] for word in model.index2word  if \"api\" in word]\n",
    "youxi_embeddings = [model[word] for word in model.index2word  if \"youxi\" in word or \"game\" in word ]\n",
    "media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "travel_embeddings = [model[word] for word in model.index2word  if \"travel\" in word or \"trip\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aliyun_embeddings = [model[word] for word in model.index2word  if \"aliyun\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ya.tmall.com',\n",
       " 'm.tmall.com',\n",
       " 'jdy.tmall.com',\n",
       " 'a.m.tmall.com',\n",
       " 's.click.tmall.com',\n",
       " 'www.tmall.com',\n",
       " 's.m.tmall.com',\n",
       " 'smc.tmall.com',\n",
       " 'cart.m.tmall.com',\n",
       " 'login.m.tmall.com',\n",
       " 'pcookie.tmall.com',\n",
       " 'page.m.tmall.com',\n",
       " 'yyz.m.tmall.com',\n",
       " 'detail.tmall.com',\n",
       " 'twp.m.tmall.com',\n",
       " 'go.m.tmall.com',\n",
       " 'list.tmall.com',\n",
       " 'pass.tmall.com',\n",
       " 'buy.m.tmall.com',\n",
       " 'bar.tmall.com',\n",
       " 'chaoshi.m.tmall.com',\n",
       " 'aldcdn.tmall.com',\n",
       " 'webww.tmall.com',\n",
       " 'err.tmall.com',\n",
       " 'mdetail.tmall.com',\n",
       " 'dsr.rate.tmall.com',\n",
       " 'api.m.tmall.com',\n",
       " 'rate.tmall.com',\n",
       " 'shop.m.tmall.com',\n",
       " 'fav.m.tmall.com',\n",
       " 'buy.tmall.com',\n",
       " 'page.twp.tmall.com',\n",
       " 'mybrand.tmall.com',\n",
       " 'vip.tmall.com',\n",
       " 'gongxiao.tmall.com',\n",
       " 'tuikuan.tmall.com',\n",
       " 'vivo.m.tmall.com',\n",
       " 'uniqlo.m.tmall.com',\n",
       " 'neo.tmall.com',\n",
       " 'trade.tmall.com',\n",
       " 'levis.tmall.com',\n",
       " 'z.tmall008.com',\n",
       " 'd.m.tmall.com',\n",
       " 'fbuy.tmall.com',\n",
       " 'only.m.tmall.com',\n",
       " 'levis.m.tmall.com',\n",
       " 'cart.tmall.com',\n",
       " 'www.025tmall.com',\n",
       " 'detail.m.tmall.com',\n",
       " 'semir.m.tmall.com',\n",
       " 'nike.m.tmall.com',\n",
       " 'chaoshi.tmall.com',\n",
       " 'handuyishe.m.tmall.com',\n",
       " 'brand.m.tmall.com',\n",
       " 'adidas.m.tmall.com',\n",
       " 'metersbonwe.m.tmall.com',\n",
       " 'brand.tmall.com',\n",
       " 'hf.m.tmall.com',\n",
       " 'sanzhisongshu.m.tmall.com',\n",
       " 'romon.m.tmall.com',\n",
       " 'jackjones.m.tmall.com',\n",
       " 'r.m.tmall.com',\n",
       " 'refund.tmall.com',\n",
       " 'yztsyx.m.tmall.com',\n",
       " 'inman.m.tmall.com',\n",
       " 'laiyifen.m.tmall.com',\n",
       " 'etam.m.tmall.com',\n",
       " 'newbalance.m.tmall.com',\n",
       " 'qiushuiyiren.m.tmall.com',\n",
       " 'ripfs.m.tmall.com',\n",
       " 'jumbougg.m.tmall.com',\n",
       " 'ruiao.m.tmall.com',\n",
       " 'miroy.m.tmall.com',\n",
       " 'balabala.m.tmall.com',\n",
       " 'jeanswest.m.tmall.com',\n",
       " 'gap.m.tmall.com',\n",
       " 'ratewrite.tmall.com',\n",
       " 'chuyu.m.tmall.com',\n",
       " 'piaolingdashu.m.tmall.com',\n",
       " 'yunnantechan.m.tmall.com',\n",
       " 'pouillylegende.m.tmall.com',\n",
       " 'pg.tmall.com',\n",
       " 'threecolour.m.tmall.com',\n",
       " 'jinyuannz.m.tmall.com',\n",
       " 'artka.m.tmall.com',\n",
       " 'sentubila.m.tmall.com',\n",
       " 'meadjohnson.tmall.com',\n",
       " '189.m.tmall.com',\n",
       " 'fiveplus.m.tmall.com',\n",
       " 'decathlon.m.tmall.com',\n",
       " 'nanjirentl.m.tmall.com',\n",
       " 'ferrero.m.tmall.com',\n",
       " 'binf.m.tmall.com',\n",
       " 'login.tmall.com',\n",
       " 'mobile.tmall.com',\n",
       " 'veromoda.m.tmall.com',\n",
       " 'teenieweenie.m.tmall.com',\n",
       " 'hengyuanxiangbby.tmall.com',\n",
       " 'brand1.tmall.com',\n",
       " 'tonlion.m.tmall.com',\n",
       " 'tiantian.m.tmall.com',\n",
       " '361du.m.tmall.com',\n",
       " 'ochirly.m.tmall.com',\n",
       " 'xpyd.m.tmall.com',\n",
       " 'qianmeiyuan.m.tmall.com',\n",
       " 'lining.m.tmall.com',\n",
       " 'nongge.m.tmall.com',\n",
       " 'pg.m.tmall.com',\n",
       " 'ebelle.m.tmall.com',\n",
       " 'vivo.tmall.com',\n",
       " 'mudifs.m.tmall.com',\n",
       " 'shinenany.m.tmall.com',\n",
       " 'fruitday.tmall.com',\n",
       " 'cz.m.tmall.com',\n",
       " '1727.m.tmall.com',\n",
       " 'rights.tmall.com',\n",
       " 'loreal.m.tmall.com',\n",
       " 'yaojingdekoudai.m.tmall.com',\n",
       " 'jinyuejj.m.tmall.com',\n",
       " 'camelnz.m.tmall.com',\n",
       " 'me-city.m.tmall.com',\n",
       " 'sffs.m.tmall.com',\n",
       " 'ruibeika.m.tmall.com',\n",
       " 'heilanhome.m.tmall.com',\n",
       " 'daphne.m.tmall.com',\n",
       " 'thefaceshop.m.tmall.com',\n",
       " 'auman.m.tmall.com',\n",
       " 'chaoshi.detail.tmall.com',\n",
       " 'langsha.m.tmall.com',\n",
       " 'ttwlswysp.m.tmall.com',\n",
       " 'gmlsp.m.tmall.com',\n",
       " 'yishengyuan.m.tmall.com',\n",
       " 'estii.m.tmall.com',\n",
       " 'luotuo.m.tmall.com',\n",
       " 'edenpure.m.tmall.com',\n",
       " 'fotile.m.tmall.com',\n",
       " 'bosideng.m.tmall.com',\n",
       " 'chowtaifook.m.tmall.com',\n",
       " 'goelia.m.tmall.com',\n",
       " 'kipone.m.tmall.com',\n",
       " 'osa.m.tmall.com',\n",
       " 'gxg.m.tmall.com',\n",
       " 'basichouse.m.tmall.com',\n",
       " 'liby.tmall.com',\n",
       " 'davebella.m.tmall.com',\n",
       " 'microsoftstore.m.tmall.com',\n",
       " 'yalu.m.tmall.com',\n",
       " 'gap.tmall.com',\n",
       " 'nestle.m.tmall.com',\n",
       " 'microsoftstore.tmall.com',\n",
       " 'tmcc.tmall.com',\n",
       " 'meadjohnson.m.tmall.com',\n",
       " 'dasandeyu.m.tmall.com',\n",
       " 'kmax.m.tmall.com',\n",
       " 'elegantprosper.tmall.com',\n",
       " 'haibangmy.m.tmall.com',\n",
       " 'xiaomi.m.tmall.com',\n",
       " 'bejirog.m.tmall.com',\n",
       " 'samsung.m.tmall.com',\n",
       " 'tanmujiang.m.tmall.com',\n",
       " 'selected.m.tmall.com',\n",
       " 'qianzhihe.m.tmall.com',\n",
       " 'lianhelihua.m.tmall.com',\n",
       " 'fairwhale.m.tmall.com',\n",
       " 'laopai.m.tmall.com',\n",
       " 'winshare.tmall.com',\n",
       " 'ukyo.m.tmall.com',\n",
       " 'ripfs.tmall.com',\n",
       " 'tianxiangsh.m.tmall.com',\n",
       " 'yearcon.m.tmall.com',\n",
       " 'ruxi.m.tmall.com',\n",
       " 'ygcp.m.tmall.com',\n",
       " 'olay.m.tmall.com',\n",
       " 'newbalance-tmall.com',\n",
       " 'pwts.m.tmall.com',\n",
       " '3m.m.tmall.com',\n",
       " 'yunifang.m.tmall.com',\n",
       " 'hotwind.m.tmall.com',\n",
       " 'ladyangel.m.tmall.com',\n",
       " 'converse.m.tmall.com',\n",
       " 'tcldq.m.tmall.com',\n",
       " 'cityliferx.m.tmall.com',\n",
       " 'dixism.m.tmall.com',\n",
       " 'sdeer.m.tmall.com',\n",
       " 'joeone.m.tmall.com',\n",
       " 'kangximuye.m.tmall.com',\n",
       " 'yobeyi.m.tmall.com',\n",
       " 'jackjones.tmall.com',\n",
       " 'hongyuanxin.tmall.com',\n",
       " 'laopai.tmall.com',\n",
       " 'epson.tmall.com',\n",
       " 'chikoolmt.m.tmall.com',\n",
       " 'othermix.m.tmall.com',\n",
       " 'toread.m.tmall.com',\n",
       " 'northland.m.tmall.com',\n",
       " 'xinjumingjia.m.tmall.com',\n",
       " 'suxingdeleyuan.m.tmall.com',\n",
       " 'www.tmall-cnas.com',\n",
       " 'ganso.m.tmall.com',\n",
       " 'eskin.m.tmall.com',\n",
       " 'zippo.tmall.com',\n",
       " 'miaoxin.tmall.com',\n",
       " 'shitingluyaomn.m.tmall.com',\n",
       " 'anzhufs.m.tmall.com',\n",
       " 'kakashizixiu.m.tmall.com',\n",
       " 'tcldq.tmall.com',\n",
       " 'mb.m.tmall.com',\n",
       " '9i.m.tmall.com',\n",
       " 'belle.m.tmall.com',\n",
       " 'rocksm.m.tmall.com',\n",
       " 'vattizm.tmall.com',\n",
       " 'lagogo.m.tmall.com',\n",
       " 'img.tmallcdn.com',\n",
       " 'heinz.m.tmall.com',\n",
       " 'asusbjb.m.tmall.com',\n",
       " 'tangchenbeijian.m.tmall.com',\n",
       " 'yiyexing.m.tmall.com',\n",
       " 'kstjj.m.tmall.com',\n",
       " 'angelcitiz.m.tmall.com',\n",
       " 'triph5.m.tmall.com',\n",
       " 'oppo.m.tmall.com',\n",
       " 'ashimabjh.m.tmall.com',\n",
       " 'nmsm.m.tmall.com',\n",
       " 'mdqsdq.m.tmall.com',\n",
       " 'epson.m.tmall.com',\n",
       " 'eral.m.tmall.com',\n",
       " 'mfsj1908.m.tmall.com',\n",
       " 'loreal.tmall.com',\n",
       " 'viscap.m.tmall.com',\n",
       " 'koukoufu.m.tmall.com',\n",
       " 'chowsangsang.m.tmall.com',\n",
       " 'playboyny.m.tmall.com',\n",
       " 'runyiyifs.m.tmall.com',\n",
       " 'tmallsns.taobao.com',\n",
       " 'meizu.m.tmall.com',\n",
       " 'loveesteem.m.tmall.com',\n",
       " 'lshmy.tmall.com',\n",
       " 'lafaso.m.tmall.com',\n",
       " 'www.canadagoose-tmall.com',\n",
       " 'kbird.m.tmall.com',\n",
       " 'nollmet.m.tmall.com',\n",
       " 'junyujj.m.tmall.com',\n",
       " 'baiyiya.m.tmall.com',\n",
       " 'skii.m.tmall.com',\n",
       " 'gurunvanifs.m.tmall.com',\n",
       " 'mohanyimei.m.tmall.com',\n",
       " 'sdeerconcept.m.tmall.com',\n",
       " 'hodohome.m.tmall.com',\n",
       " 'shsiemens.m.tmall.com',\n",
       " 'pb89.m.tmall.com',\n",
       " 'cachecachebmnw.m.tmall.com',\n",
       " 'esprit.m.tmall.com',\n",
       " 'triplenineny.m.tmall.com',\n",
       " 'umdnj.m.tmall.com',\n",
       " 'wuzhoushipin.m.tmall.com',\n",
       " 'jealousy.m.tmall.com',\n",
       " 'camelsports.m.tmall.com',\n",
       " 'jonaswagell.m.tmall.com',\n",
       " 'achette.m.tmall.com',\n",
       " 'feishengmy.m.tmall.com',\n",
       " 'huawei.m.tmall.com',\n",
       " 'nautica.m.tmall.com',\n",
       " 'deesha.m.tmall.com',\n",
       " 'ziwei.m.tmall.com',\n",
       " 'richricas.m.tmall.com',\n",
       " 'lzfur.m.tmall.com',\n",
       " 'ssbsm.m.tmall.com',\n",
       " 'isido.m.tmall.com',\n",
       " 'fanersen.m.tmall.com',\n",
       " 'kaqiwu.m.tmall.com',\n",
       " 'metersbonwe.tmall.com',\n",
       " 'xychjj.m.tmall.com',\n",
       " 'chjjewellery.m.tmall.com',\n",
       " 'romon.tmall.com',\n",
       " 'kaifandiml.m.tmall.com',\n",
       " 'mumuhome.m.tmall.com',\n",
       " 'amh.m.tmall.com',\n",
       " 'omifs.m.tmall.com',\n",
       " 'easterncamel.m.tmall.com',\n",
       " 'brita.m.tmall.com',\n",
       " 'kao.m.tmall.com',\n",
       " 'yearconnx.m.tmall.com',\n",
       " 'htshuma.m.tmall.com',\n",
       " 'fmart.tmall.com',\n",
       " 'tdwfs.m.tmall.com',\n",
       " 'balenojd.tmall.com',\n",
       " 'lenovo.m.tmall.com',\n",
       " 'a02.m.tmall.com',\n",
       " 'afwj.m.tmall.com',\n",
       " 'nuoleidun.m.tmall.com',\n",
       " 'xiaotao.m.tmall.com',\n",
       " 'qiyaqcyp.m.tmall.com',\n",
       " 'yuzhaolinzj.m.tmall.com',\n",
       " 'ebcfs.m.tmall.com',\n",
       " 'angrybirds.tmall.com',\n",
       " 'clioborn.tmall.com',\n",
       " 'honb.m.tmall.com',\n",
       " 'ansels.m.tmall.com',\n",
       " 'www.jackwolfskin-tmall.com',\n",
       " 'nanjiren.m.tmall.com',\n",
       " 'ugiz.m.tmall.com',\n",
       " 'lingwuqiyishijia.m.tmall.com',\n",
       " 'yiwensp.m.tmall.com',\n",
       " 'zuoluoxiansheng.tmall.com',\n",
       " 'yixiangliying.m.tmall.com',\n",
       " 'antakids.tmall.com',\n",
       " 'fuanna.m.tmall.com',\n",
       " 'artka.tmall.com',\n",
       " 'angrybirds.m.tmall.com',\n",
       " 'jplus.m.tmall.com',\n",
       " 'nbastore.m.tmall.com',\n",
       " 'hqyt.m.tmall.com',\n",
       " 'lailaimy.m.tmall.com',\n",
       " 'shunvfang.m.tmall.com',\n",
       " 'dphone.m.tmall.com',\n",
       " 'forge.m.tmall.com',\n",
       " 'snowforte.m.tmall.com',\n",
       " 'nanjirenfh.m.tmall.com',\n",
       " 'mikibana.m.tmall.com',\n",
       " 'fensejisql.m.tmall.com',\n",
       " 'jiazhuli.m.tmall.com',\n",
       " 'mxm.m.tmall.com',\n",
       " 'liby.m.tmall.com',\n",
       " 'beibeixiong.m.tmall.com',\n",
       " 'fanqi.m.tmall.com',\n",
       " 'adoodoo.m.tmall.com',\n",
       " 'xlxsp.m.tmall.com',\n",
       " 'toplanyde.m.tmall.com',\n",
       " 'gebaosz.m.tmall.com',\n",
       " 'hems.m.tmall.com',\n",
       " 'ka.tmall.com',\n",
       " 'httz.m.tmall.com',\n",
       " 'honb.tmall.com',\n",
       " 'jasonwood.m.tmall.com',\n",
       " 'sankins.m.tmall.com',\n",
       " 'watsons.tmall.com',\n",
       " 'zbwj.m.tmall.com',\n",
       " 'donoraticogf.m.tmall.com',\n",
       " 'xinranjj.m.tmall.com',\n",
       " 'brita.tmall.com',\n",
       " 'bkyd.m.tmall.com',\n",
       " 'senda.m.tmall.com',\n",
       " 'nanjirenyz.m.tmall.com',\n",
       " 'afsjeep.m.tmall.com',\n",
       " 'bejirogmy.tmall.com',\n",
       " 'ojays.m.tmall.com',\n",
       " 'yijialun.m.tmall.com',\n",
       " 'waayes.m.tmall.com',\n",
       " 'dabuwawa.m.tmall.com',\n",
       " 'ygcp.tmall.com',\n",
       " 'gradyboy.m.tmall.com',\n",
       " 'dingdangxiaozhu.m.tmall.com',\n",
       " 'lego.m.tmall.com',\n",
       " 'suyujj.m.tmall.com',\n",
       " 'aimiss.m.tmall.com',\n",
       " 'powerdekor.tmall.com',\n",
       " 'queend.m.tmall.com',\n",
       " 'hasbro.m.tmall.com',\n",
       " 'hezhifs.m.tmall.com',\n",
       " '27ri.m.tmall.com',\n",
       " 'elle.m.tmall.com',\n",
       " 'cmzx.m.tmall.com',\n",
       " 'leistress.m.tmall.com',\n",
       " 'fmart.m.tmall.com',\n",
       " 'luys.m.tmall.com',\n",
       " 'xinchaojj.tmall.com',\n",
       " 'dove.m.tmall.com',\n",
       " 'uniqlo.tmall.com',\n",
       " 'yidunjide.m.tmall.com',\n",
       " 'baomanfs.m.tmall.com',\n",
       " 'qiantx.m.tmall.com',\n",
       " 'toread.tmall.com',\n",
       " 'czcbsts.m.tmall.com',\n",
       " 'turnsignal.m.tmall.com',\n",
       " 'fgsm.m.tmall.com',\n",
       " 'suppod.m.tmall.com',\n",
       " 'zhenchuanfs.m.tmall.com',\n",
       " 'uniwal.tmall.com',\n",
       " 'ymygfs.m.tmall.com',\n",
       " 'xrui.m.tmall.com',\n",
       " 'staccato.m.tmall.com',\n",
       " 'jfxysp.m.tmall.com',\n",
       " 'neutrogena.m.tmall.com',\n",
       " 'mexicannb.m.tmall.com',\n",
       " '1828nanzhuang.m.tmall.com',\n",
       " 'erke.m.tmall.com',\n",
       " 'lvsensm.m.tmall.com',\n",
       " 'rmmfs.m.tmall.com',\n",
       " 'lsspring.m.tmall.com',\n",
       " 'songzhuts.m.tmall.com',\n",
       " 'busen.m.tmall.com',\n",
       " 'vivala.m.tmall.com',\n",
       " 'xxcwyp.m.tmall.com',\n",
       " 'malilan.m.tmall.com',\n",
       " 'manhua.m.tmall.com',\n",
       " 'eoshzp.tmall.com',\n",
       " 'beely.m.tmall.com',\n",
       " 'cbanner.m.tmall.com',\n",
       " 'suuntohwmj.m.tmall.com',\n",
       " 'moen.m.tmall.com',\n",
       " 'eminencestoney.m.tmall.com',\n",
       " 'yiluodi.m.tmall.com',\n",
       " 'eifini.m.tmall.com',\n",
       " 'bkny.m.tmall.com',\n",
       " 'kufei.m.tmall.com',\n",
       " 'wandian.m.tmall.com',\n",
       " 'haier.m.tmall.com',\n",
       " 'bosidengnyy.m.tmall.com',\n",
       " 'mojay.m.tmall.com',\n",
       " 'lxdxb.m.tmall.com',\n",
       " 'tmall.com',\n",
       " 'peacebird.m.tmall.com',\n",
       " 'maxfactor.m.tmall.com',\n",
       " 'hangyige.m.tmall.com',\n",
       " 'fuguixiong.m.tmall.com',\n",
       " 'shehexw.tmall.com',\n",
       " 'yanazi.m.tmall.com',\n",
       " 'qinghuayang.m.tmall.com',\n",
       " 'wyqcyp.m.tmall.com',\n",
       " 'qjzm.tmall.com',\n",
       " 'xiaoxiaohai.m.tmall.com',\n",
       " 'ltnx.m.tmall.com',\n",
       " 'lmfs.m.tmall.com',\n",
       " 'rrjjry.tmall.com',\n",
       " 'qxnwhf.m.tmall.com',\n",
       " 'delivery.tmall.com',\n",
       " 'gtjj.m.tmall.com',\n",
       " 'eland.m.tmall.com',\n",
       " 'siyake.m.tmall.com',\n",
       " 'bonjung.m.tmall.com',\n",
       " 'haoxiangni.m.tmall.com',\n",
       " 'hems.tmall.com',\n",
       " 'supfiresh.m.tmall.com',\n",
       " 'ruiao.tmall.com',\n",
       " 'okko.m.tmall.com',\n",
       " 'kipone.tmall.com',\n",
       " 'xinnengliang.m.tmall.com',\n",
       " 'syoss.m.tmall.com',\n",
       " 'yifengfs.m.tmall.com',\n",
       " 'cofco.m.tmall.com',\n",
       " 'saintgirl.m.tmall.com',\n",
       " 'baicaowei.m.tmall.com',\n",
       " 'qilekang.tmall.com',\n",
       " 'nicilq.m.tmall.com',\n",
       " 'yytsm.tmall.com',\n",
       " 'jindasheng.m.tmall.com',\n",
       " 'joyoungtp.m.tmall.com',\n",
       " 'adidas.tmall.com',\n",
       " 'thermos.m.tmall.com',\n",
       " 'qiaosha.m.tmall.com',\n",
       " 'pisenhzj.m.tmall.com',\n",
       " 'lwxl.m.tmall.com',\n",
       " 'carenzdz.m.tmall.com',\n",
       " 'gainreel.m.tmall.com',\n",
       " 'yihaodian.m.tmall.com',\n",
       " 'xicunmingwu.m.tmall.com',\n",
       " 'lotionspa.m.tmall.com',\n",
       " 'hzxs.m.tmall.com',\n",
       " 'zhijinlou.m.tmall.com',\n",
       " 'juxingyihaosrt.m.tmall.com',\n",
       " 'liangzuxl.m.tmall.com',\n",
       " 'junlinyd.m.tmall.com',\n",
       " 'goodbaby.m.tmall.com',\n",
       " 'aiken.m.tmall.com',\n",
       " 'zhenbeihz.m.tmall.com']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in model.index2word  if \"tmall\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count----> 100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'media_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-75706e1fe207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count---->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedia_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mshop_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmap_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'media_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "for count in [100,200,300,400,500,700,800]:\n",
    "    print(\"count---->\",count)\n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42)\n",
    "    clf_SVM = SVM.fit(x_train, y_train)\n",
    "    doc_class_predicted = clf_SVM.predict(x_test)\n",
    "    print(np.mean(doc_class_predicted == y_test))\n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count----> 100\n",
      "62 0\n",
      "99 0\n",
      "83 0\n",
      "0.8133333333333334\n",
      "count----> 200\n",
      "187 2\n",
      "189 2\n",
      "107 1\n",
      "0.805\n",
      "count----> 300\n",
      "172 0\n",
      "297 0\n",
      "188 2\n",
      "0.73\n",
      "count----> 400\n",
      "379 1\n",
      "398 1\n",
      "172 1\n",
      "0.7908333333333334\n",
      "count----> 500\n",
      "382 0\n",
      "444 0\n",
      "241 0\n",
      "0.7113333333333334\n",
      "count----> 700\n",
      "563 0\n",
      "472 0\n",
      "131 0\n",
      "0.5552380952380952\n",
      "count----> 800\n",
      "761 1\n",
      "510 1\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "no mode for empty data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-2fc57f856835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/statistics.py\u001b[0m in \u001b[0;36mmode\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 )\n\u001b[1;32m    476\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no mode for empty data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStatisticsError\u001b[0m: no mode for empty data"
     ]
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "for count in [100,200,300,400,500,700,800]:\n",
    "    print(\"count---->\",count)\n",
    "    final_embeddings= media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "    len(final_embeddings)\n",
    "    from sklearn import cluster, datasets\n",
    "    X = final_embeddings\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "#     print(k_means.labels_[::])\n",
    "#     print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ),mode(k_means.labels_[:count:]))\n",
    "    print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]),mode(k_means.labels_[count:count+count:]))\n",
    "    print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]),mode(k_means.labels_[count+count:count+count+count:]))\n",
    "\n",
    "    print(right_count/(3*count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count----> 100\n",
      "0.94\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.93      0.93        54\n",
      "          1       0.93      0.95      0.94        44\n",
      "          2       0.94      0.94      0.94        52\n",
      "\n",
      "avg / total       0.94      0.94      0.94       150\n",
      "\n",
      "count----> 200\n",
      "0.956666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95        97\n",
      "          1       0.99      0.93      0.96       107\n",
      "          2       0.94      0.97      0.95        96\n",
      "\n",
      "avg / total       0.96      0.96      0.96       300\n",
      "\n",
      "count----> 300\n",
      "0.926666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.93       154\n",
      "          1       0.91      0.97      0.94       153\n",
      "          2       0.94      0.87      0.91       143\n",
      "\n",
      "avg / total       0.93      0.93      0.93       450\n",
      "\n",
      "count----> 400\n",
      "0.93\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93       204\n",
      "          1       0.92      0.98      0.95       198\n",
      "          2       0.95      0.88      0.92       198\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "count----> 500\n",
      "0.918666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.94      0.93       263\n",
      "          1       0.87      0.95      0.91       239\n",
      "          2       0.97      0.87      0.92       248\n",
      "\n",
      "avg / total       0.92      0.92      0.92       750\n",
      "\n",
      "count----> 700\n",
      "0.920952380952\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.93      0.92       346\n",
      "          1       0.90      0.92      0.91       357\n",
      "          2       0.97      0.91      0.94       347\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1050\n",
      "\n",
      "count----> 800\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [2341 2400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-c804cb3e66a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [2341 2400]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for count in [100,200,300,400,500,700,800]:\n",
    "    print(\"count---->\",count)\n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    clf_SVM = SVM.fit(x_train, y_train)\n",
    "    doc_class_predicted = clf_SVM.predict(x_test)\n",
    "    print(np.mean(doc_class_predicted == y_test))\n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88013601,  0.849029  ,  2.06500793,  1.47481406,  1.05131602,\n",
       "       -3.23556709, -1.422328  , -0.66254503, -2.1743691 , -1.54976594,\n",
       "        2.05652595,  0.91469598,  2.60806298, -1.54639602, -1.68464506,\n",
       "       -2.83229899, -0.90186101, -1.67103899, -2.12337589,  0.98171002,\n",
       "        1.29474699,  1.37360597, -2.80476189,  0.908171  ,  1.66404498,\n",
       "        0.87784398, -0.29884499,  0.422692  ,  1.24100804,  4.40271807,\n",
       "       -1.06018102, -1.56210601, -2.58844805, -1.67301202,  3.1986351 ,\n",
       "        0.53300899,  0.188034  , -1.94363403,  0.81556201, -0.3985    ,\n",
       "        1.75758898,  1.45460296, -1.87927902, -1.28549194, -1.89206004,\n",
       "       -1.11207497,  0.43751499,  0.82267702,  1.56893802, -0.16922501,\n",
       "        2.41652894, -0.19342899,  0.97147399,  0.37545401, -1.00442803,\n",
       "        0.235394  , -2.95109606,  1.45366204, -1.11558998,  1.81515205,\n",
       "       -0.06049   ,  1.00733197,  3.85380006, -1.06283998, -0.92553002,\n",
       "        1.78450596, -1.25091398,  0.53140301, -1.37926805, -0.25111601,\n",
       "        0.90266103, -1.23709297,  1.61233497, -0.49246201,  2.79231405,\n",
       "        0.19640701, -0.090707  , -0.39385599, -1.477525  , -1.30097604,\n",
       "        0.200883  , -0.95764601,  2.01641202, -0.18790901,  1.44120705,\n",
       "       -1.52755296,  0.238389  ,  0.261163  ,  1.90750802, -0.84829497,\n",
       "       -0.106365  ,  1.738397  ,  1.38577199, -0.22723299,  0.352063  ,\n",
       "       -0.183576  , -0.89239699, -0.74943399,  0.238685  , -0.99775201], dtype=float32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "clf_SVM = SVM.fit(x_train, y_train)\n",
    "doc_class_predicted = clf_SVM.predict(x_test)\n",
    "print(np.mean(doc_class_predicted == y_test))\n",
    "print(metrics.classification_report(y_test, doc_class_predicted))\n",
    "    \n",
    "for i in range(10):\n",
    "    print(clf_SVM.predict(aliyun_embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose best windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size1\n",
      "[ 0.96658312  1.          0.96658312  1.          1.          1.          1.\n",
      "  1.          1.          1.          0.96658312  1.          0.93460925\n",
      "  1.          0.90194084  0.96658312  0.96658312  0.96658312  1.\n",
      "  0.96658312]\n",
      "Accuracy: 0.98 (+/- 0.05) \n",
      "\n",
      "train-model-size2\n",
      "[ 0.96658312  0.96658312  0.96658312  0.96658312  1.          0.96658312\n",
      "  0.96658312  1.          1.          1.          0.93333333  1.\n",
      "  0.96658312  1.          0.86436488  1.          0.93265993  0.96658312\n",
      "  1.          0.96658312]\n",
      "Accuracy: 0.97 (+/- 0.07) \n",
      "\n",
      "train-model-size3\n",
      "[ 0.89500042  0.93121693  0.8359183   0.96658312  0.8993266   0.96658312\n",
      "  0.96658312  0.96658312  0.96658312  1.          0.96658312  1.\n",
      "  0.96658312  0.89952153  0.86228956  1.          0.96658312  0.96658312\n",
      "  1.          0.89952153]\n",
      "Accuracy: 0.95 (+/- 0.09) \n",
      "\n",
      "train-model-size5\n",
      "[ 1.          0.8994709   0.93460925  0.96658312  0.89952153  0.93460925\n",
      "  0.96658312  1.          0.96658312  1.          0.90150376  1.\n",
      "  0.96658312  0.93265993  0.86772487  1.          0.96658312  0.96658312\n",
      "  1.          0.96658312]\n",
      "Accuracy: 0.96 (+/- 0.08) \n",
      "\n",
      "train-model-size7\n",
      "[ 0.96658312  0.93121693  0.93460925  0.96658312  0.8994709   1.\n",
      "  0.93460925  1.          0.93265993  1.          0.86482874  0.96658312\n",
      "  0.96658312  0.96658312  0.89952153  0.96658312  0.96658312  0.93121693\n",
      "  1.          0.8994709 ]\n",
      "Accuracy: 0.95 (+/- 0.08) \n",
      "\n",
      "train-model-size9\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window9/model-all-vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-401d4fb6ece4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# for i in [100]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train-model-size%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-word2vec\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-vocabulary\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbaidu_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0;34m\"baidu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading word counts from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# local files -- both read & write supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Get an S3 host. It is required for sigv4 operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jw/anaconda3/lib/python3.5/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_closing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window9/model-all-vocabulary'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [1,2,3,5,7,9,13,15,20,30,50]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\"\\n\")\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [1,2,3,5,7,9,13,15,20,30,50]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 100\n",
    "    sina_embeddings = [model[word] for word in model.index2word  if \"sina\" in word]\n",
    "    wangyi_embeddings = [model[word] for word in model.index2word  if \"163\" in word ]\n",
    "    youku_embeddings = [model[word] for word in model.index2word  if \"youku\" in word ]\n",
    "#     media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),'\\n')\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size1\n",
      "[ 0.56825397  0.77128427  0.87037037  1.          0.86666667  0.86666667\n",
      "  1.          0.93265993  0.93265993  0.93265993  0.79545455  0.93265993\n",
      "  0.85606061  0.79191919  1.          0.93265993  0.87037037  0.86666667\n",
      "  0.93265993  0.86599327]\n",
      "Accuracy: 0.88 (+/- 0.19) \n",
      "\n",
      "train-model-size2\n",
      "[ 0.58201058  0.93265993  0.93265993  0.71572872  0.93265993  0.80538721\n",
      "  0.93265993  0.93265993  0.93265993  0.86666667  0.79444444  1.\n",
      "  0.85606061  0.79191919  0.93265993  1.          0.93265993  0.86666667\n",
      "  1.          1.        ]\n",
      "Accuracy: 0.89 (+/- 0.21) \n",
      "\n",
      "train-model-size3\n",
      "[ 0.29131653  0.77128427  1.          0.73484848  0.86599327  0.80538721\n",
      "  0.85606061  0.80538721  0.93265993  0.93265993  0.73484848  1.\n",
      "  0.93265993  0.86666667  0.93265993  1.          1.          0.79444444\n",
      "  0.93265993  0.93265993]\n",
      "Accuracy: 0.86 (+/- 0.31) \n",
      "\n",
      "train-model-size5\n",
      "[ 0.51111111  0.77128427  0.72525253  0.73131313  0.86666667  0.86666667\n",
      "  0.77128427  0.86599327  0.80538721  0.79191919  0.79545455  1.\n",
      "  0.86111111  0.72222222  1.          0.85606061  0.93265993  0.79191919\n",
      "  1.          1.        ]\n",
      "Accuracy: 0.83 (+/- 0.24) \n",
      "\n",
      "train-model-size7\n",
      "[ 0.5         0.86599327  0.93265993  0.80538721  0.86599327  0.86599327\n",
      "  0.77128427  0.73131313  0.93265993  0.86599327  0.79444444  1.\n",
      "  0.86111111  0.72222222  0.87037037  0.93265993  0.93265993  0.72222222\n",
      "  1.          0.93265993]\n",
      "Accuracy: 0.85 (+/- 0.23) \n",
      "\n",
      "train-model-size10\n",
      "[ 0.49766573  0.77128427  0.93265993  0.87037037  0.80270655  0.86666667\n",
      "  0.93265993  0.86666667  0.93265993  0.93265993  0.72222222  0.93265993\n",
      "  0.79545455  0.73888889  0.93265993  0.93265993  0.93265993  0.86599327\n",
      "  0.87037037  1.        ]\n",
      "Accuracy: 0.86 (+/- 0.22) \n",
      "\n",
      "train-model-size13\n",
      "[ 0.49603175  0.86666667  1.          0.7962963   0.80538721  0.7962963\n",
      "  0.93265993  0.86599327  0.93265993  0.93265993  0.80270655  1.\n",
      "  0.93265993  0.72863248  1.          1.          0.86111111  0.80270655\n",
      "  0.93265993  0.80270655]\n",
      "Accuracy: 0.86 (+/- 0.23) \n",
      "\n",
      "train-model-size15\n",
      "[ 0.62962963  0.6547619   0.93265993  0.86111111  0.79191919  0.7962963   1.\n",
      "  1.          0.93265993  0.85606061  0.72863248  1.          0.80270655\n",
      "  0.7962963   0.93265993  0.93265993  0.80270655  0.86599327  0.93265993\n",
      "  0.85606061]\n",
      "Accuracy: 0.86 (+/- 0.21) \n",
      "\n",
      "train-model-size20\n",
      "[ 0.61587302  0.67735043  0.65555556  0.80538721  0.66666667  0.7962963\n",
      "  0.93265993  0.86599327  0.86666667  0.86599327  0.73484848  0.93265993\n",
      "  0.85606061  0.64529915  0.93265993  0.86111111  0.79444444  0.79191919\n",
      "  0.86111111  0.93265993]\n",
      "Accuracy: 0.80 (+/- 0.20) \n",
      "\n",
      "train-model-size30\n",
      "[ 0.62962963  0.79545455  0.93265993  0.93265993  0.73809524  0.80538721\n",
      "  0.93265993  0.72222222  0.79191919  0.87037037  0.80270655  1.\n",
      "  0.79191919  0.47765568  0.93265993  0.86111111  0.77128427  0.93265993\n",
      "  0.87037037  1.        ]\n",
      "Accuracy: 0.83 (+/- 0.25) \n",
      "\n",
      "train-model-size50\n",
      "[ 0.43290043  0.77128427  0.93265993  0.93265993  0.73872054  0.8\n",
      "  0.93265993  0.73888889  0.79191919  1.          0.79191919  0.86666667\n",
      "  0.77128427  0.66464646  0.93265993  0.71355311  0.71355311  0.93265993\n",
      "  0.93265993  0.7962963 ]\n",
      "Accuracy: 0.81 (+/- 0.26) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [1,2,3,5,7,10,13,15,20,30,50]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-window%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 100\n",
    "    shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "    map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "    media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "#     media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),'\\n')\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose the best model-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25573948  0.42350427  0.41488779  0.48798328  0.45382503  0.47992554\n",
      "  0.46714702  0.5047081   0.53189103  0.47676768  0.54300214  0.44170519\n",
      "  0.46337279  0.44323671  0.44218926  0.45831193  0.44038429  0.45489377\n",
      "  0.48210923  0.47446515]\n",
      "Accuracy: 0.46 (+/- 0.11) \n",
      "\n",
      "train-model-size5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95972256  0.96        0.97333333  0.66571989  0.94632313  0.86557474\n",
      "  0.76772852  0.93294613  0.91849705  0.87926774  0.91853925  0.93354265\n",
      "  0.82089264  0.89081869  0.89239057  0.93127453  0.8922414   0.91866376\n",
      "  0.87885776  0.83444444]\n",
      "Accuracy: 0.89 (+/- 0.14) \n",
      "\n",
      "train-model-size10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95985548  0.88012507  0.94753086  0.93322315  0.973328    0.93333333\n",
      "  0.95995726  0.95995726  0.93324786  0.93380342  0.93354265  0.90709381\n",
      "  0.89376068  0.89197531  0.93397296  0.92044596  0.90733074  0.89446691\n",
      "  0.86861629  0.8676391 ]\n",
      "Accuracy: 0.92 (+/- 0.06) \n",
      "\n",
      "train-model-size15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9465812   0.9732906   0.98666133  0.97357405  0.98666133  0.89339773\n",
      "  1.          0.97357405  0.93275772  0.93304522  0.94606838  0.97333333\n",
      "  0.93380342  0.90632313  0.90482002  0.93390874  0.85539216  0.83974359\n",
      "  0.80246914  0.77324263]\n",
      "Accuracy: 0.92 (+/- 0.12) \n",
      "\n",
      "train-model-size20\n",
      "[ 0.98666133  0.93453149  0.95936694  0.94632313  0.98666133  0.89482906\n",
      "  1.          0.96044047  0.9465812   0.94689542  0.9599686   0.9194847\n",
      "  0.94636316  0.90880222  0.90752243  0.94607843  0.90711669  0.88104042\n",
      "  0.84110902  0.77324263]\n",
      "Accuracy: 0.93 (+/- 0.10) \n",
      "\n",
      "train-model-size50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95985548  0.89461783  0.93324786  0.973328    0.93385087  0.9212782\n",
      "  0.97303922  0.94722933  0.9602561   0.84346505  0.98666133  0.94665066\n",
      "  0.94683502  0.94666667  0.94636316  0.93318881  0.87995964  0.88188829\n",
      "  0.9465812   0.81060606]\n",
      "Accuracy: 0.93 (+/- 0.09) \n",
      "\n",
      "train-model-size80\n",
      "[ 0.95972256  0.94665066  0.88149573  0.98666133  0.93324786  0.82397306\n",
      "  0.98666133  0.93322958  0.95972256  0.86716287  0.9732906   0.93397296\n",
      "  0.95995726  0.9599686   0.95972256  0.94636316  0.89139256  0.86863724\n",
      "  0.94722933  0.89053093]\n",
      "Accuracy: 0.93 (+/- 0.09) \n",
      "\n",
      "train-model-size100\n",
      "[ 0.94689542  0.91975309  0.94683502  1.          0.91997332  0.84042821\n",
      "  0.98666133  0.93294613  0.96        0.89429618  0.97303922  0.96044047\n",
      "  0.93333333  0.95995726  0.94638922  0.94683502  0.82700101  0.93364198\n",
      "  0.90691856  0.87596355]\n",
      "Accuracy: 0.93 (+/- 0.09) \n",
      "\n",
      "train-model-size200\n",
      "[ 0.93354265  0.89324786  0.85333333  0.98666133  0.89184636  0.88057802\n",
      "  0.90653846  0.90684051  0.89269231  0.88172164  0.97357405  0.94537347\n",
      "  0.93255295  0.9602561   0.93236203  0.95995726  0.86942555  0.95936694\n",
      "  0.94655158  0.87745283]\n",
      "Accuracy: 0.92 (+/- 0.08) \n",
      "\n",
      "train-model-size500\n",
      "[ 0.90709381  0.7820911   0.86828285  0.94713675  0.87935897  0.85375084\n",
      "  0.94638922  0.94753086  0.94713675  0.93453149  1.          0.9193266\n",
      "  0.96044047  0.97357405  0.9732906   0.90599327  0.89309241  0.9732906\n",
      "  0.93322958  0.87598181]\n",
      "Accuracy: 0.92 (+/- 0.10) \n",
      "\n",
      "train-model-size1000\n",
      "[ 0.90711669  0.85405261  0.84145299  0.9732906   0.87624104  0.86601307\n",
      "  0.92        0.91975309  0.94714809  0.86842917  0.98666133  0.95985548\n",
      "  0.93127453  0.94692277  0.94561062  0.94662393  0.87962963  0.94655158\n",
      "  0.90616269  0.86111111]\n",
      "Accuracy: 0.91 (+/- 0.08) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 500\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\"\\n\")\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17824074  0.21893815  0.21900161  0.35217674  0.40740741  0.48829431\n",
      "  0.49378399  0.48240166  0.63369963  0.4378907   0.48444444  0.52408702\n",
      "  0.63990676  0.65117845  0.50974026  0.64385965  0.51233155  0.27540361\n",
      "  0.60185185  0.44117647]\n",
      "Accuracy: 0.46 (+/- 0.29) \n",
      "\n",
      "train-model-size5\n",
      "[ 1.          1.          0.83467001  1.          0.96658312  0.96658312\n",
      "  0.93265993  0.93324979  0.89769821  1.          0.96658312  0.93265993\n",
      "  0.93460925  1.          0.93460925  0.93265993  0.93265993  0.86469739\n",
      "  0.83786762  0.93460925]\n",
      "Accuracy: 0.94 (+/- 0.10) \n",
      "\n",
      "train-model-size10\n",
      "[ 1.          1.          0.90150376  0.96658312  0.93265993  1.          1.\n",
      "  0.93324979  1.          0.96658312  1.          0.93121693  0.96658312\n",
      "  1.          0.93324979  0.93333333  0.93460925  0.86469739  0.86599327\n",
      "  0.93460925]\n",
      "Accuracy: 0.95 (+/- 0.09) \n",
      "\n",
      "train-model-size15\n",
      "[ 1.          1.          0.93460925  1.          1.          1.          1.\n",
      "  0.93265993  1.          1.          1.          0.89500042  0.93460925\n",
      "  0.96658312  0.93121693  0.90194084  0.89952153  0.89816207  0.93333333\n",
      "  1.        ]\n",
      "Accuracy: 0.96 (+/- 0.08) \n",
      "\n",
      "train-model-size20\n",
      "[ 1.          1.          0.8993266   1.          1.          0.96658312\n",
      "  1.          0.96658312  1.          1.          0.93324979  0.93265993\n",
      "  0.93460925  0.8994709   0.93121693  0.96658312  0.96658312  0.89816207\n",
      "  0.9         0.96658312]\n",
      "Accuracy: 0.96 (+/- 0.08) \n",
      "\n",
      "train-model-size50\n",
      "[ 0.96658312  1.          0.93121693  0.83198653  0.96658312  1.          1.\n",
      "  0.96658312  0.93121693  1.          0.96658312  0.86436488  0.8359183\n",
      "  0.83413078  0.96658312  0.93324979  0.90150376  0.83786762  0.86902357\n",
      "  0.93460925]\n",
      "Accuracy: 0.93 (+/- 0.12) \n",
      "\n",
      "train-model-size80\n",
      "[ 0.89974937  0.96658312  0.93324979  0.93324979  1.          0.93333333\n",
      "  0.93333333  1.          0.96658312  0.8993266   0.93324979  0.96658312\n",
      "  0.8359183   0.96658312  1.          0.93265993  0.93324979  0.90150376\n",
      "  0.96658312  0.90150376]\n",
      "Accuracy: 0.94 (+/- 0.08) \n",
      "\n",
      "train-model-size100\n",
      "[ 0.96658312  1.          1.          0.8993266   1.          1.          1.\n",
      "  1.          0.96658312  1.          0.93324979  0.90194084  0.90150376\n",
      "  0.93460925  0.90150376  0.96658312  0.89816207  0.89952153  0.8993266\n",
      "  0.93460925]\n",
      "Accuracy: 0.95 (+/- 0.09) \n",
      "\n",
      "train-model-size200\n",
      "[ 0.96658312  0.96658312  0.89952153  0.83642439  0.96658312  0.96658312\n",
      "  0.96658312  0.90194084  0.93324979  1.          0.93460925  0.93324979\n",
      "  0.83642439  0.96658312  0.96658312  0.96658312  0.96658312  0.8994709\n",
      "  0.93324979  0.93121693]\n",
      "Accuracy: 0.94 (+/- 0.09) \n",
      "\n",
      "train-model-size500\n",
      "[ 0.83295136  0.93324979  0.83569024  0.90194084  0.89500042  0.93460925\n",
      "  0.8993266   1.          0.96658312  1.          0.93333333  0.83747277\n",
      "  0.79799499  0.90150376  0.8993266   0.96658312  0.96658312  0.8994709\n",
      "  0.86817043  0.93265993]\n",
      "Accuracy: 0.91 (+/- 0.11) \n",
      "\n",
      "train-model-size1000\n",
      "[ 0.86469739  0.80555556  0.90150376  0.80542986  0.93121693  0.89974937\n",
      "  0.86772487  0.93121693  0.93121693  0.96658312  0.89769821  0.83198653\n",
      "  0.83467001  0.89816207  0.93324979  0.96658312  0.96658312  0.8994709\n",
      "  0.83642439  0.93460925]\n",
      "Accuracy: 0.90 (+/- 0.10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\"\\n\")\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05128205  0.04444444  0.28438228  0.36111111  0.45396825  0.36825397\n",
      "  0.44047619  0.28042328  0.14814815  0.28240741  0.24019608  0.11764706\n",
      "  0.20740741  0.3627451   0.28654971  0.23611111  0.25925926  0.16666667\n",
      "  0.19907407  0.11111111]\n",
      "Accuracy: 0.25 (+/- 0.23) \n",
      "\n",
      "train-model-size5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72486772  0.54112554  0.51068376  0.61111111  0.48051948  0.50555556\n",
      "  0.7979798   0.86111111  0.74074074  0.30952381  0.73131313  0.66666667\n",
      "  0.80270655  0.44607843  0.33333333  0.45396825  0.72486772  0.28654971\n",
      "  0.37566138  0.78021978]\n",
      "Accuracy: 0.58 (+/- 0.36) \n",
      "\n",
      "train-model-size10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.86111111  0.71937322  1.          0.80270655  0.87037037\n",
      "  0.93265993  0.93265993  1.          0.52996633  1.          0.66227106\n",
      "  0.86599327  0.87037037  0.79444444  0.50462963  0.87037037  0.93265993\n",
      "  0.78021978  0.87037037]\n",
      "Accuracy: 0.84 (+/- 0.28) \n",
      "\n",
      "train-model-size15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          0.93265993  1.          0.79191919  1.\n",
      "  0.93265993  0.93265993  0.93265993  0.7962963   0.93265993  0.87037037\n",
      "  0.7962963   0.79191919  1.          0.93265993  0.86111111  1.\n",
      "  0.93265993  1.        ]\n",
      "Accuracy: 0.92 (+/- 0.15) \n",
      "\n",
      "train-model-size20\n",
      "[ 0.68253968  0.93265993  0.87037037  0.93265993  0.72486772  1.\n",
      "  0.93265993  1.          0.93265993  0.8         0.86666667  0.93265993\n",
      "  0.93265993  0.85606061  0.86111111  0.79191919  0.72777778  0.7979798   1.\n",
      "  1.        ]\n",
      "Accuracy: 0.88 (+/- 0.19) \n",
      "\n",
      "train-model-size50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86111111  0.86666667  0.93265993  1.          0.85606061  1.\n",
      "  0.86666667  0.93265993  0.93265993  0.85606061  0.87037037  0.86599327\n",
      "  1.          0.93265993  0.86111111  0.79191919  0.93265993  0.86666667\n",
      "  0.86111111  1.        ]\n",
      "Accuracy: 0.90 (+/- 0.12) \n",
      "\n",
      "train-model-size80\n",
      "[ 1.          1.          1.          0.93265993  0.7962963   0.93265993\n",
      "  0.93265993  0.93265993  0.86599327  0.73484848  0.93265993  0.93265993\n",
      "  1.          0.93265993  0.86599327  1.          1.          1.          1.\n",
      "  0.93265993]\n",
      "Accuracy: 0.94 (+/- 0.14) \n",
      "\n",
      "train-model-size100\n",
      "[ 1.          0.93265993  0.86111111  0.87037037  0.86666667  1.\n",
      "  0.86599327  0.93265993  0.7962963   0.86599327  0.93265993  0.93265993\n",
      "  0.93265993  0.93265993  0.93265993  0.85606061  0.93265993  0.87037037\n",
      "  1.          1.        ]\n",
      "Accuracy: 0.92 (+/- 0.11) \n",
      "\n",
      "train-model-size200\n",
      "[ 1.          0.93265993  0.86599327  0.93265993  0.80538721  0.78021978\n",
      "  0.8         0.93265993  0.86666667  0.79444444  0.85606061  1.\n",
      "  0.93265993  0.93265993  0.93265993  0.85606061  1.          0.7979798\n",
      "  0.86111111  0.93265993]\n",
      "Accuracy: 0.89 (+/- 0.14) \n",
      "\n",
      "train-model-size500\n",
      "[ 0.86111111  0.93265993  0.7962963   0.93265993  0.79444444  0.93265993\n",
      "  0.72222222  1.          0.86599327  0.80270655  0.86111111  0.93265993\n",
      "  1.          0.86599327  1.          0.87037037  0.93265993  0.93265993\n",
      "  0.86111111  1.        ]\n",
      "Accuracy: 0.89 (+/- 0.15) \n",
      "\n",
      "train-model-size1000\n",
      "[ 0.63369963  0.78021978  0.8         0.85606061  0.87037037  0.7962963\n",
      "  0.71572872  0.93265993  0.86599327  0.79545455  0.80270655  1.          1.\n",
      "  1.          1.          0.93265993  0.93265993  0.93265993  0.87037037\n",
      "  1.        ]\n",
      "Accuracy: 0.88 (+/- 0.20) \n",
      "\n",
      "train-model-size2000\n",
      "[ 0.86666667  0.86599327  0.80270655  0.93265993  0.86599327  0.71572872\n",
      "  0.72525253  1.          0.93265993  0.87037037  0.77128427  1.          1.\n",
      "  0.93265993  1.          0.86599327  0.86111111  0.86666667  0.79545455\n",
      "  1.        ]\n",
      "Accuracy: 0.88 (+/- 0.18) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000,2000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 100\n",
    "    shop_embeddings = [model[word] for word in model.index2word  if \"sina\" in word]\n",
    "    map_embeddings = [model[word] for word in model.index2word  if \"163\" in word ]\n",
    "    media_embeddings = [model[word] for word in model.index2word  if \"uc\" in word ]\n",
    "#     media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),'\\n')\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27318296  0.25426945  0.19876543  0.22114118  0.2951505   0.15937149\n",
      "  0.41385621  0.24082935  0.35824916  0.37082903  0.41563461  0.24675325\n",
      "  0.35714286  0.36199095  0.3502886   0.16512346  0.28150043  0.35978836\n",
      "  0.29088851  0.14935065]\n",
      "Accuracy: 0.29 (+/- 0.16) \n",
      "\n",
      "train-model-size5\n",
      "[ 0.30219114  0.43915344  0.36        0.16666667  0.48340548  0.60930513\n",
      "  0.50925926  0.39712919  0.50722394  0.50864198  0.4973545   0.43611111\n",
      "  0.4973545   0.4304929   0.53333333  0.34915825  0.63950617  0.3651946\n",
      "  0.54558405  0.41920216]\n",
      "Accuracy: 0.45 (+/- 0.21) \n",
      "\n",
      "train-model-size10\n",
      "[ 0.72354312  0.59267399  0.70135747  0.52768488  0.58131868  0.61719298\n",
      "  0.83295136  0.64480432  0.66942356  0.80133668  0.86443381  0.83150836\n",
      "  0.69124579  0.83150836  0.78611111  0.75277778  0.72222222  0.69694617\n",
      "  0.60219298  0.73285872]\n",
      "Accuracy: 0.71 (+/- 0.19) \n",
      "\n",
      "train-model-size15\n",
      "[ 0.54592789  0.83298934  0.56925647  0.6522807   0.68333333  0.89816207\n",
      "  0.75833333  0.8         0.66397306  0.80133668  0.83413078  0.7709023\n",
      "  0.76161616  0.86436488  0.89769821  0.69972032  0.86228956  0.80542986\n",
      "  0.65407108  0.82514919]\n",
      "Accuracy: 0.76 (+/- 0.20) \n",
      "\n",
      "train-model-size20\n",
      "[ 0.62603175  0.79181586  0.75925926  0.80083058  0.64516684  0.86243386\n",
      "  0.80204604  0.83786762  0.79229267  0.76608187  0.74120083  0.86325439\n",
      "  0.76379354  0.86243386  0.83245614  0.68552632  0.76054664  0.69561158\n",
      "  0.57774065  0.90150376]\n",
      "Accuracy: 0.77 (+/- 0.17) \n",
      "\n",
      "train-model-size50\n",
      "[ 0.62222222  0.79444444  0.72277396  0.6978279   0.89500042  0.93121693\n",
      "  0.73116124  0.8359183   0.93265993  0.83786762  0.90150376  0.8993266\n",
      "  0.73537937  0.79365079  0.79888126  0.86599327  0.86685312  0.79444444\n",
      "  0.82575758  0.86325439]\n",
      "Accuracy: 0.82 (+/- 0.16) \n",
      "\n",
      "train-model-size80\n",
      "[ 0.73673203  0.71320346  0.72317115  0.75378788  0.76198964  0.83198653\n",
      "  0.86641604  0.76599587  0.8994709   0.93265993  0.86443381  0.86641604\n",
      "  0.83467001  0.89974937  0.86436488  0.80542986  0.79790162  0.76111111\n",
      "  0.82556936  0.8993266 ]\n",
      "Accuracy: 0.82 (+/- 0.13) \n",
      "\n",
      "train-model-size100\n",
      "[ 0.71944444  0.75491718  0.80133668  0.79377365  0.76290727  0.93460925\n",
      "  0.76531987  0.83333333  0.8994709   0.82832988  0.7989418   0.93333333\n",
      "  0.7687127   0.86817043  0.87037037  0.90150376  0.83103154  0.80542986\n",
      "  0.8151076   0.83298934]\n",
      "Accuracy: 0.83 (+/- 0.12) \n",
      "\n",
      "train-model-size200\n",
      "[ 0.65251898  0.86436488  0.76464646  0.80083058  0.82974102  0.78480457\n",
      "  0.86641604  0.86436488  0.8994709   0.89500042  0.96658312  0.90150376\n",
      "  0.86469739  0.78888889  0.93460925  0.8317654   0.8993266   0.82575758\n",
      "  0.82575758  0.93324979]\n",
      "Accuracy: 0.85 (+/- 0.14) \n",
      "\n",
      "train-model-size500\n",
      "[ 0.58902692  0.76783626  0.76574286  0.89974937  0.82548171  0.85731539\n",
      "  0.76161616  0.86469739  0.83068783  0.86436488  0.96658312  0.96658312\n",
      "  0.90150376  0.86807706  0.8993266   0.83103154  0.87037037  0.79382333\n",
      "  0.86111111  0.93324979]\n",
      "Accuracy: 0.85 (+/- 0.17) \n",
      "\n",
      "train-model-size1000\n",
      "[ 0.61777778  0.75769133  0.83413078  0.86641604  0.96658312  0.93121693\n",
      "  0.79444444  0.86325439  1.          0.85731539  0.93324979  0.96658312\n",
      "  0.83537937  0.79825708  0.80043573  0.87037037  0.80043573  0.86436488\n",
      "  0.79545455  0.79078947]\n",
      "Accuracy: 0.85 (+/- 0.17) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    \n",
    "    shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "    map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "    media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),'\\n')\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model-size2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44675926  0.40972222  0.36984127  0.23931624  0.33868093  0.38861522\n",
      "  0.34666667  0.37179487  0.42911877  0.21153846  0.13468013  0.35164835\n",
      "  0.33149758  0.39924812  0.22962963  0.19457014  0.29813665  0.42698413\n",
      "  0.37777778  0.46388029]\n",
      "Accuracy: 0.34 (+/- 0.18) \n",
      "\n",
      "train-model-size5\n",
      "[ 0.53491908  0.52853398  0.45125949  0.51390013  0.54126362  0.52325686\n",
      "  0.75215666  0.75491718  0.4962963   0.61333333  0.4722881   0.45072574\n",
      "  0.30805728  0.53838299  0.43221691  0.50366657  0.46444444  0.63231032\n",
      "  0.59766082  0.35396825]\n",
      "Accuracy: 0.52 (+/- 0.22) \n",
      "\n",
      "train-model-size10\n",
      "[ 0.68736842  0.86807706  0.66564365  0.66214178  0.76835017  0.76719577\n",
      "  0.56111111  0.62897886  0.76835017  0.68821549  0.86817043  0.71048999\n",
      "  0.72200436  0.55555556  0.57936508  0.50083595  0.56746032  0.62962963\n",
      "  0.68444444  0.45228456]\n",
      "Accuracy: 0.67 (+/- 0.22) \n",
      "\n",
      "train-model-size15\n",
      "[ 0.7986532   0.79377365  0.7687127   0.72348485  0.6936181   0.72361466\n",
      "  0.66268328  0.78888889  0.76874003  0.72828283  0.83333333  0.8027799\n",
      "  0.76769218  0.90150376  0.76566952  0.74074074  0.6547619   0.70313037\n",
      "  0.67217062  0.6547619 ]\n",
      "Accuracy: 0.75 (+/- 0.13) \n",
      "\n",
      "train-model-size20\n",
      "[ 0.58444816  0.56520932  0.70138358  0.63561077  0.86443381  0.63042187\n",
      "  0.656122    0.74120083  0.8993266   0.73015873  0.738437    0.83132832\n",
      "  0.738437    0.96658312  0.61210966  0.75378788  0.62229437  0.77205387\n",
      "  0.73561404  0.6654386 ]\n",
      "Accuracy: 0.72 (+/- 0.21) \n",
      "\n",
      "train-model-size50\n",
      "[ 0.89500042  0.86228956  0.83295136  0.77376906  0.83198653  0.89816207\n",
      "  0.7986532   0.83068783  0.7989418   0.96658312  0.89974937  0.79904306\n",
      "  0.86641604  0.89816207  0.72636166  0.8994709   0.7687127   0.83642439\n",
      "  0.73484848  0.7628139 ]\n",
      "Accuracy: 0.83 (+/- 0.12) \n",
      "\n",
      "train-model-size80\n",
      "[ 0.66349206  0.83068783  0.73291562  0.69028133  0.72886762  0.86666667\n",
      "  0.86599327  0.69028133  0.83597884  0.83786762  0.86228956  0.8993266\n",
      "  0.77376906  0.79974937  0.6545584   0.90194084  0.75793651  0.80270655\n",
      "  0.76851852  0.71937322]\n",
      "Accuracy: 0.78 (+/- 0.15) \n",
      "\n",
      "train-model-size100\n",
      "[ 0.60935143  0.77012384  0.76574286  0.78888889  0.72760943  0.83295136\n",
      "  0.79904306  0.86490381  0.73333333  0.8994709   0.8692185   0.93324979\n",
      "  0.86490381  0.93265993  0.62891738  0.83747277  0.72551603  0.69373219\n",
      "  0.73333333  0.79790162]\n",
      "Accuracy: 0.79 (+/- 0.18) \n",
      "\n",
      "train-model-size200\n",
      "[ 0.73455338  0.83413078  0.72222222  0.79640769  0.83132832  0.78612209\n",
      "  0.89816207  0.76719577  0.82981609  0.83413078  0.83298934  0.80133668\n",
      "  0.75769133  0.89974937  0.68778281  0.86817043  0.78888889  0.83747277\n",
      "  0.73333333  0.7687127 ]\n",
      "Accuracy: 0.80 (+/- 0.11) \n",
      "\n",
      "train-model-size500\n",
      "[ 0.39885815  0.86243386  0.74074074  0.93460925  0.75581906  0.93333333\n",
      "  0.8993266   0.76515152  0.82895623  0.8993266   0.86666667  0.86641604\n",
      "  0.73333333  0.8692185   0.6547619   0.8993266   0.6674264   0.86111111\n",
      "  0.76111111  0.76111111]\n",
      "Accuracy: 0.80 (+/- 0.25) \n",
      "\n",
      "train-model-size1000\n",
      "[ 0.50805861  0.75833333  0.75848252  0.83068783  0.86599327  0.8993266\n",
      "  0.86641604  0.75581906  0.7962963   0.90150376  1.          0.8993266\n",
      "  0.87037037  0.90194084  0.6547619   0.93460925  0.74285714  0.96658312\n",
      "  0.83103154  0.69969499]\n",
      "Accuracy: 0.82 (+/- 0.23) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import shutil \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "# for i in [100]:\n",
    "    print(\"train-model-size%s\"%str(i))\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/repeat-model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    \n",
    "    shop_embeddings = [model[word] for word in model.index2word  if \"shop\" in word]\n",
    "    map_embeddings = [model[word] for word in model.index2word  if \"map\" in word ]\n",
    "    media_embeddings = [model[word] for word in model.index2word  if \"music\" in word or \"video\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = media_embeddings[:count]+shop_embeddings[:count]+map_embeddings[:count]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=234)\n",
    "\n",
    "\n",
    "    SVM=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=20, random_state=42)\n",
    "    scores = cross_val_score(SVM, X, y, cv=20,scoring='f1_macro')\n",
    "    print(scores)                                              \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),'\\n')\n",
    "#     clf_SVM = SVM.fit(x_train, y_train)\n",
    "#     doc_class_predicted = clf_SVM.predict(x_test)\n",
    "#     print(np.mean(doc_class_predicted == y_test))\n",
    "#     print(metrics.classification_report(y_test, doc_class_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "for i in [2,5,10,15,20,50,80,100,200,500,1000]:\n",
    "    output_model_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size\"+str(i)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-word2vec\"%str(i), fvocab=\"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/result_jw_new/myline/model-size%s/model-all-vocabulary\"%str(i), binary=False)\n",
    "    count = 200\n",
    "    baidu_embeddings = [model[word] for word in model.index2word  if \"baidu\" in word]\n",
    "    taobao_embeddings = [model[word] for word in model.index2word  if \"taobao\" in word]\n",
    "    qq_embeddings = [model[word] for word in model.index2word  if \"qq\" in word]\n",
    "    \n",
    "    \n",
    "    y = np.array([0 for i in range(count)]+[1 for i in range(count)]+[2 for i in range(count)])\n",
    "    X = baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "    \n",
    "    final_embeddings=baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "#     len(final_embeddings)\n",
    "    from sklearn import cluster, datasets\n",
    "    X = final_embeddings\n",
    "    k_means = cluster.KMeans(n_clusters=3)\n",
    "    k_means.fit(X)\n",
    "    # print(k_means.labels_[::])\n",
    "    # print(len(k_means.labels_[::]))\n",
    "\n",
    "    right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "    # print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "    # print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "    # print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "    print(i,\"------>>>>\",right_count/(3*count),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 1 2 0 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 2 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1\n",
      " 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 2 1 0 1 0 0 1 1 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 2 2 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
      " 1 0 2 1 1 2 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 2 1 1 1 1 2 1 2 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 2 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 2 1 1 0 1 0 0 1 1 1\n",
      " 1 1 1 2 1 1 0 1]\n",
      "600\n",
      "185\n",
      "196\n",
      "138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "count = 200\n",
    "final_embeddings=baidu_embeddings[:count]+taobao_embeddings[:count]+qq_embeddings[:count]\n",
    "len(final_embeddings)\n",
    "from sklearn import cluster, datasets\n",
    "X = final_embeddings\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X)\n",
    "print(k_means.labels_[::])\n",
    "print(len(k_means.labels_[::]))\n",
    "\n",
    "right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) \n",
    "print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "\n",
    "right_count/(3*count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 1 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 2 2 3 3 1\n",
      " 3 3 3 3 3 1 3 3 1 3 3 3 3 3 3 0 3 3 3 1 3 3 3 3 3 3 2 3 3 3 3 1 1 3 0 3 3\n",
      " 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2\n",
      " 3 1 2 3 3 2 3 2 3 1 3 3 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 1 1 2 2 2 1 2 2 3 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1\n",
      " 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 3 2 2 1 2 2 2 1 2 2 2 2 0 2 1 1 3 2 3 1 1 2 2 1 2 2 2 2 1 2 1 1\n",
      " 1 3 2 2 2 1 1 2 2 2 2 2 2 2 3 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 1 2 2 2 2 2 2 1 2 2 2 2 1 1 1 2 2 1 1 1 2 2 2 1 2 1 2 2 1 1 2 2 2 1 2 2 1\n",
      " 1 2 2 2 2 1 2 1 1 2 2 2 2 2 2 1 2 1 2 1 2 1 2 2 3 1 2 2 2 2 1 2 2 2 1 2 2\n",
      " 2 2 2 0 2 3 2 2 2 3 2 1 1 1 3 3 1 3 2 1 2 2 2 2 1 2 2 3 2 2 3 2 2 2 3 1 2\n",
      " 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 0 1 0 1 0\n",
      " 1 3 0 1 2 2 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 2 1 1 2 1 1 1 1 2 1 2 1 1 1 2\n",
      " 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 0 1 1 1 1 2 1 1 1 1 1 3 0 1 1 1 1 1 3 0 1\n",
      " 1 1 2 3 1 0 1 2 0 1 2 2 2 1 1 1 1 1 1 1 2 0 1 3 2 3 1 1 2 2 1 1 1 1 2 2 1\n",
      " 3 1 2 3 2 2 2 0 1 1 1 1 1 1 1 2 2 0 1 0 1 0 2 3 1 1 2 2 1 2 1 2 0 3 1 2 1\n",
      " 1 3 2 3 2 1 1 1 2 1 3 3 2 2 1 1 2 3 3 1 2 1 1 0 2 0 2 0 0 0 2 0 2 0 0 2 0\n",
      " 2 0 2 0 0 1 0 0 2 2 1 2 0 0 2 0 0 3 0 0 0 0 0 0 0 0 0 2 0 0 2 0 1 0 0 0 0\n",
      " 2 0 0 0 0 0 2 0 1 0 3 0 3 1 0 0 0 1 0 1 0 0 0 0 0 2 0 0 0 0 0 0 3 2 0 3 0\n",
      " 0 0 0 0 1 0 0 0 0 2 0 0 0 0 2 1 1 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0\n",
      " 0 2 0 0 0 0 0 0 0 2 0 0 0 0 1 0 2 0 2 0 2 1 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0\n",
      " 2 1 0 0 2 0 0 0 0 2 0 2 0 0 2 2 0 2 0 1 1 0 0 2 0 2 0 0 0 0 0 0 0 3 0 0 0\n",
      " 0]\n",
      "1000\n",
      "619\n",
      "172\n",
      "187\n",
      "136\n",
      "124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "count = 200\n",
    "final_embeddings=baidu_embeddings[:count]+taobao_embeddings[:count]+sina_embeddings[:count]+uc_embeddings[:count]+qq_embeddings[:count]\n",
    "len(final_embeddings)\n",
    "from sklearn import cluster, datasets\n",
    "X = final_embeddings\n",
    "k_means = cluster.KMeans(n_clusters=4)\n",
    "k_means.fit(X)\n",
    "print(k_means.labels_[::])\n",
    "print(len(k_means.labels_[::]))\n",
    "\n",
    "right_count=len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] )+len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])])+len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]) + len([i for i in k_means.labels_[count+count+count:count+count+count+count:] if i==mode(k_means.labels_[count+count+count:count+count+count+count:])]) \n",
    "\n",
    "print(right_count)\n",
    "print(len([i for i in k_means.labels_[:count:] if i==mode(k_means.labels_[:count:])] ))\n",
    "print(len([i for i in k_means.labels_[count:count+count:] if i==mode(k_means.labels_[count:count+count:])]))\n",
    "print(len([i for i in k_means.labels_[count+count:count+count+count:] if i==mode(k_means.labels_[count+count:count+count+count:])]))\n",
    "print(len([i for i in k_means.labels_[count+count+count:count+count+count+count:] if i==mode(k_means.labels_[count+count+count:count+count+count+count:])]) )\n",
    "\n",
    "right_count/(4*count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode(k_means.labels_[:count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('update.xyq.163.com', 0.6822214126586914),\n",
       " ('res.xyq.cbg.163.com', 0.6381403207778931),\n",
       " ('123.58.170.12', 0.6181110143661499),\n",
       " ('xy2-android.cbg.163.com', 0.5335538387298584),\n",
       " ('xyq-ios.cbg.163.com', 0.5245332717895508),\n",
       " ('183.61.2.94', 0.5020437240600586),\n",
       " ('service.mkey.163.com', 0.4834635853767395),\n",
       " ('123.58.173.180', 0.47402223944664),\n",
       " ('lbs.dd.163.com', 0.46957576274871826),\n",
       " ('res.nie.netease.com', 0.46432632207870483)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"xyq-android.cbg.163.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cn.patch.battle.net', 0.9757399559020996),\n",
       " ('client02.pdl.wow.battlenet.com.cn', 0.9654581546783447),\n",
       " ('dist.blizzard.com.edgesuite.net', 0.9624665975570679),\n",
       " ('zhcn.patch.battle.net', 0.9200407266616821),\n",
       " ('cn.launcher.battlenet.com.cn', 0.9119179248809814),\n",
       " ('cn-test.patch.battle.net', 0.8966389298439026),\n",
       " ('public-test.patch.battle.net', 0.8699407577514648),\n",
       " ('client01.pdl.wow.battlenet.com.cn', 0.8346480131149292),\n",
       " ('template.wps.cn', 0.8248226642608643),\n",
       " ('client03.pdl.wow.battlenet.com.cn', 0.8204830884933472)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"iir.blizzard.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client02.pdl.wow.battlenet.com.cn', 0.978783130645752),\n",
       " ('iir.blizzard.com', 0.9757399559020996),\n",
       " ('dist.blizzard.com.edgesuite.net', 0.9612374901771545),\n",
       " ('zhcn.patch.battle.net', 0.9320716261863708),\n",
       " ('cn.launcher.battlenet.com.cn', 0.9257140159606934),\n",
       " ('cn-test.patch.battle.net', 0.9091231822967529),\n",
       " ('public-test.patch.battle.net', 0.8897701501846313),\n",
       " ('client01.pdl.wow.battlenet.com.cn', 0.8683668375015259),\n",
       " ('client03.pdl.wow.battlenet.com.cn', 0.8408011198043823),\n",
       " ('template.wps.cn', 0.7996641397476196)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"cn.patch.battle.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"client02.pdl.wow.battlenet.com.cn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"pdl\" in a  or \"02\"  in a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
